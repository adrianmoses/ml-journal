{
  
    
        "post0": {
            "title": "AI and ML for Coders Ch 2",
            "content": "Introduction to Computer Vision . The ability to algorithmicly see an image as a type of clothing is very difficult to define with rule-based programming. Instead, let&#39;s use machine learning. . Using the Fashion MNIST dataset we have 10 types of images based on clothing types. Each image is 28x28 and is in black and white. Meaning each pixel value is between 0 and 255. . We can&#39;t model a linear relationship between the X (image pixels) and the Y (clothing type) . However we can use the output node layer as a represenation of the 10 clothing types. Each image will be loaded into every node and the output will spit out a probability that the image is of the clothing type the output node represents. . !pip install tensorflow-cpu . Collecting tensorflow-cpu Downloading tensorflow_cpu-2.6.0-cp38-cp38-macosx_10_11_x86_64.whl (199.0 MB) |████████████████████████████████| 199.0 MB 105 kB/s Requirement already satisfied: typing-extensions~=3.7.4 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-cpu) (3.7.4.3) Collecting keras~=2.6 Using cached keras-2.6.0-py2.py3-none-any.whl (1.3 MB) Collecting astunparse~=1.6.3 Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB) Collecting termcolor~=1.1.0 Using cached termcolor-1.1.0.tar.gz (3.9 kB) Collecting gast==0.4.0 Using cached gast-0.4.0-py3-none-any.whl (9.8 kB) Collecting google-pasta~=0.2 Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB) Requirement already satisfied: wheel~=0.35 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-cpu) (0.36.2) Collecting h5py~=3.1.0 Downloading h5py-3.1.0-cp38-cp38-macosx_10_9_x86_64.whl (2.9 MB) |████████████████████████████████| 2.9 MB 29.6 MB/s Requirement already satisfied: wrapt~=1.12.1 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-cpu) (1.12.1) Collecting absl-py~=0.10 Downloading absl_py-0.14.0-py3-none-any.whl (131 kB) |████████████████████████████████| 131 kB 34.0 MB/s Collecting keras-preprocessing~=1.1.2 Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB) Collecting opt-einsum~=3.3.0 Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB) Collecting numpy~=1.19.2 Downloading numpy-1.19.5-cp38-cp38-macosx_10_9_x86_64.whl (15.6 MB) |████████████████████████████████| 15.6 MB 28.5 MB/s Collecting tensorboard~=2.6 Using cached tensorboard-2.6.0-py3-none-any.whl (5.6 MB) Collecting grpcio&lt;2.0,&gt;=1.37.0 Downloading grpcio-1.40.0-cp38-cp38-macosx_10_10_x86_64.whl (4.0 MB) |████████████████████████████████| 4.0 MB 20.9 MB/s Collecting tensorflow-estimator~=2.6 Using cached tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB) Collecting protobuf&gt;=3.9.2 Downloading protobuf-3.18.0-cp38-cp38-macosx_10_9_x86_64.whl (1.0 MB) |████████████████████████████████| 1.0 MB 27.9 MB/s Collecting flatbuffers~=1.12.0 Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB) Requirement already satisfied: six~=1.15.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-cpu) (1.15.0) Collecting clang~=5.0 Using cached clang-5.0.tar.gz (30 kB) Requirement already satisfied: requests&lt;3,&gt;=2.21.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6-&gt;tensorflow-cpu) (2.25.1) Requirement already satisfied: werkzeug&gt;=0.11.15 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6-&gt;tensorflow-cpu) (1.0.1) Collecting markdown&gt;=2.6.8 Using cached Markdown-3.3.4-py3-none-any.whl (97 kB) Requirement already satisfied: setuptools&gt;=41.0.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6-&gt;tensorflow-cpu) (52.0.0.post20210125) Collecting tensorboard-plugin-wit&gt;=1.6.0 Using cached tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB) Collecting google-auth-oauthlib&lt;0.5,&gt;=0.4.1 Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB) Collecting tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 Using cached tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB) Collecting google-auth&lt;2,&gt;=1.6.3 Using cached google_auth-1.35.0-py2.py3-none-any.whl (152 kB) Collecting rsa&lt;5,&gt;=3.1.4 Using cached rsa-4.7.2-py3-none-any.whl (34 kB) Collecting cachetools&lt;5.0,&gt;=2.0.0 Using cached cachetools-4.2.2-py3-none-any.whl (11 kB) Collecting pyasn1-modules&gt;=0.2.1 Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB) Collecting requests-oauthlib&gt;=0.7.0 Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB) Collecting pyasn1&lt;0.5.0,&gt;=0.4.6 Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB) Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard~=2.6-&gt;tensorflow-cpu) (4.0.0) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard~=2.6-&gt;tensorflow-cpu) (1.26.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard~=2.6-&gt;tensorflow-cpu) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard~=2.6-&gt;tensorflow-cpu) (2020.12.5) Collecting oauthlib&gt;=3.0.0 Using cached oauthlib-3.1.1-py2.py3-none-any.whl (146 kB) Building wheels for collected packages: clang, termcolor Building wheel for clang (setup.py) ... done Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30702 sha256=da15ad6b0d6d4b2a3771249b17258a29cd98a0ec320998b59d0271947a6dc249 Stored in directory: /Users/adrianmoses/Library/Caches/pip/wheels/f1/60/77/22b9b5887bd47801796a856f47650d9789c74dc3161a26d608 Building wheel for termcolor (setup.py) ... done Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=0e700811fb720cbb45531971531821ac22f2c4752b8e8da9f111d274868e236f Stored in directory: /Users/adrianmoses/Library/Caches/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501 Successfully built clang termcolor Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, clang, astunparse, tensorflow-cpu Attempting uninstall: numpy Found existing installation: numpy 1.20.1 Uninstalling numpy-1.20.1: Successfully uninstalled numpy-1.20.1 Attempting uninstall: h5py Found existing installation: h5py 2.10.0 Uninstalling h5py-2.10.0: Successfully uninstalled h5py-2.10.0 Successfully installed absl-py-0.14.0 astunparse-1.6.3 cachetools-4.2.2 clang-5.0 flatbuffers-1.12 gast-0.4.0 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.40.0 h5py-3.1.0 keras-2.6.0 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.18.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.6.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-cpu-2.6.0 tensorflow-estimator-2.6.0 termcolor-1.1.0 . import tensorflow as tf import numpy as np from tensorflow.keras import Sequential from tensorflow.keras.layers import Dense from tensorflow import keras . data = tf.keras.datasets.fashion_mnist # dataset (training_images, training_labels), (test_images, test_labels) = data.load_data() # load data into train and test sets training_images = training_images / 255.0 # normalize the images so that they&#39;re all between 0 and 1 test_images = test_images / 255.0 model = keras.Sequential([ keras.layers.Flatten(input_shape=(28, 28)), keras.layers.Dense(128, activation=tf.nn.relu), # hidden layer keras.layers.Dense(10, activation=tf.nn.softmax) # output layer ]) model.compile(optimizer=&#39;adam&#39;, # adam is an evolution of sgd to better find that global optimum (uses momentum) loss=&#39;sparse_categorical_crossentropy&#39;, # common loss function for softmax classification metrics=[&#39;accuracy&#39;]) model.fit(training_images, training_labels, epochs=5) . Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz 32768/29515 [=================================] - 0s 0us/step 40960/29515 [=========================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz 26427392/26421880 [==============================] - 1s 0us/step 26435584/26421880 [==============================] - 1s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz 16384/5148 [===============================================================================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz 4423680/4422102 [==============================] - 0s 0us/step 4431872/4422102 [==============================] - 0s 0us/step Epoch 1/5 1875/1875 [==============================] - 2s 732us/step - loss: 0.4964 - accuracy: 0.8251 Epoch 2/5 1875/1875 [==============================] - 1s 745us/step - loss: 0.3719 - accuracy: 0.8655 Epoch 3/5 1875/1875 [==============================] - 1s 740us/step - loss: 0.3349 - accuracy: 0.8771 Epoch 4/5 1875/1875 [==============================] - 1s 743us/step - loss: 0.3114 - accuracy: 0.8846 Epoch 5/5 1875/1875 [==============================] - 1s 747us/step - loss: 0.2940 - accuracy: 0.8915 . &lt;keras.callbacks.History at 0x7ff180afcd00&gt; . model.evaluate(test_images, test_labels) . 313/313 [==============================] - 0s 508us/step - loss: 0.3397 - accuracy: 0.8791 . [0.3397229313850403, 0.8791000247001648] . classifications = model.predict(test_images) print(classifications[0]) # probabilities for each class print(test_labels[0]) # actual correct class . [1.3823378e-05 3.3052868e-07 1.6802342e-05 1.0910228e-06 6.6072988e-07 5.4068407e-03 1.5284693e-05 7.9095788e-02 3.6717476e-05 9.1541272e-01] 9 . model.fit(training_images, training_labels, epochs=50) . Epoch 1/50 1875/1875 [==============================] - 1s 749us/step - loss: 0.2797 - accuracy: 0.8959 Epoch 2/50 1875/1875 [==============================] - 1s 749us/step - loss: 0.2682 - accuracy: 0.9002 Epoch 3/50 1875/1875 [==============================] - 1s 771us/step - loss: 0.2574 - accuracy: 0.9042 Epoch 4/50 1875/1875 [==============================] - 1s 746us/step - loss: 0.2477 - accuracy: 0.9087 Epoch 5/50 1875/1875 [==============================] - 1s 746us/step - loss: 0.2379 - accuracy: 0.9117 Epoch 6/50 1875/1875 [==============================] - 1s 749us/step - loss: 0.2321 - accuracy: 0.9139 Epoch 7/50 1875/1875 [==============================] - 1s 747us/step - loss: 0.2225 - accuracy: 0.9169 Epoch 8/50 1875/1875 [==============================] - 1s 770us/step - loss: 0.2172 - accuracy: 0.9186 Epoch 9/50 1875/1875 [==============================] - 1s 770us/step - loss: 0.2100 - accuracy: 0.9210 Epoch 10/50 1875/1875 [==============================] - 1s 775us/step - loss: 0.2045 - accuracy: 0.9230 Epoch 11/50 1875/1875 [==============================] - 1s 763us/step - loss: 0.1986 - accuracy: 0.9247 Epoch 12/50 1875/1875 [==============================] - 1s 774us/step - loss: 0.1913 - accuracy: 0.9276 Epoch 13/50 1875/1875 [==============================] - 1s 749us/step - loss: 0.1880 - accuracy: 0.9282 Epoch 14/50 1875/1875 [==============================] - 1s 757us/step - loss: 0.1821 - accuracy: 0.9323 Epoch 15/50 1875/1875 [==============================] - 1s 761us/step - loss: 0.1780 - accuracy: 0.9326 Epoch 16/50 1875/1875 [==============================] - 1s 754us/step - loss: 0.1728 - accuracy: 0.9335 Epoch 17/50 1875/1875 [==============================] - 1s 768us/step - loss: 0.1700 - accuracy: 0.9364 Epoch 18/50 1875/1875 [==============================] - 1s 762us/step - loss: 0.1655 - accuracy: 0.9370 Epoch 19/50 1875/1875 [==============================] - 1s 768us/step - loss: 0.1602 - accuracy: 0.9388 Epoch 20/50 1875/1875 [==============================] - 1s 763us/step - loss: 0.1553 - accuracy: 0.9418 Epoch 21/50 1875/1875 [==============================] - 1s 761us/step - loss: 0.1539 - accuracy: 0.9417 Epoch 22/50 1875/1875 [==============================] - 1s 770us/step - loss: 0.1505 - accuracy: 0.9436 Epoch 23/50 1875/1875 [==============================] - 1s 765us/step - loss: 0.1468 - accuracy: 0.9444 Epoch 24/50 1875/1875 [==============================] - 1s 758us/step - loss: 0.1447 - accuracy: 0.9444 Epoch 25/50 1875/1875 [==============================] - 1s 795us/step - loss: 0.1399 - accuracy: 0.9472 Epoch 26/50 1875/1875 [==============================] - 1s 763us/step - loss: 0.1378 - accuracy: 0.9468 Epoch 27/50 1875/1875 [==============================] - 1s 764us/step - loss: 0.1340 - accuracy: 0.9502 Epoch 28/50 1875/1875 [==============================] - 1s 762us/step - loss: 0.1305 - accuracy: 0.9508 Epoch 29/50 1875/1875 [==============================] - 1s 757us/step - loss: 0.1293 - accuracy: 0.9516 Epoch 30/50 1875/1875 [==============================] - 1s 771us/step - loss: 0.1275 - accuracy: 0.9526 Epoch 31/50 1875/1875 [==============================] - 1s 755us/step - loss: 0.1228 - accuracy: 0.9532 Epoch 32/50 1875/1875 [==============================] - 1s 764us/step - loss: 0.1208 - accuracy: 0.9543 Epoch 33/50 1875/1875 [==============================] - 1s 752us/step - loss: 0.1166 - accuracy: 0.9552 Epoch 34/50 1875/1875 [==============================] - 1s 758us/step - loss: 0.1175 - accuracy: 0.9551 Epoch 35/50 1875/1875 [==============================] - 1s 767us/step - loss: 0.1173 - accuracy: 0.9556 Epoch 36/50 1875/1875 [==============================] - 1s 757us/step - loss: 0.1130 - accuracy: 0.9572 Epoch 37/50 1875/1875 [==============================] - 1s 763us/step - loss: 0.1120 - accuracy: 0.9571 Epoch 38/50 1875/1875 [==============================] - 1s 766us/step - loss: 0.1090 - accuracy: 0.9591 Epoch 39/50 1875/1875 [==============================] - 1s 777us/step - loss: 0.1082 - accuracy: 0.9598 Epoch 40/50 1875/1875 [==============================] - 1s 741us/step - loss: 0.1034 - accuracy: 0.9603 Epoch 41/50 1875/1875 [==============================] - 1s 770us/step - loss: 0.1031 - accuracy: 0.9622 Epoch 42/50 1875/1875 [==============================] - 1s 753us/step - loss: 0.1002 - accuracy: 0.9621 Epoch 43/50 1875/1875 [==============================] - 1s 740us/step - loss: 0.1014 - accuracy: 0.9619 Epoch 44/50 1875/1875 [==============================] - 1s 743us/step - loss: 0.0974 - accuracy: 0.9627 Epoch 45/50 1875/1875 [==============================] - 1s 747us/step - loss: 0.0962 - accuracy: 0.9638 Epoch 46/50 1875/1875 [==============================] - 1s 768us/step - loss: 0.0959 - accuracy: 0.9635 Epoch 47/50 1875/1875 [==============================] - 2s 811us/step - loss: 0.0923 - accuracy: 0.9646 Epoch 48/50 1875/1875 [==============================] - 2s 810us/step - loss: 0.0910 - accuracy: 0.9653 Epoch 49/50 1875/1875 [==============================] - 1s 787us/step - loss: 0.0909 - accuracy: 0.9661 Epoch 50/50 1875/1875 [==============================] - 2s 884us/step - loss: 0.0879 - accuracy: 0.9676 . &lt;keras.callbacks.History at 0x7ff1a125fa00&gt; . model.evaluate(test_images, test_labels) . 313/313 [==============================] - 0s 559us/step - loss: 0.5352 - accuracy: 0.8879 . [0.5351569056510925, 0.8878999948501587] . After adding 50 epochs, the training accuracy increased, but the evaluation decreased. This is a sign of overfitting because the model is having a harding time generalizing on data it hasn&#39;t seen . By the way, always recompile the model when retraining. . Let&#39;s use callbacks to train the same model but stopping it when an accuracy has been reached. . class myCallback(keras.callbacks.Callback): def on_epoch_end(self, epoch, logs={}): if logs.get(&#39;accuracy&#39;) &gt; 0.95: print(&quot; nReached 95% accuracy so cancelling training!&quot;) self.model.stop_training = True callbacks = myCallback() model = keras.models.Sequential([ keras.layers.Flatten(), keras.layers.Dense(128, activation=tf.nn.relu), keras.layers.Dense(10, activation=tf.nn.softmax) ]) model.compile(optimizer=&#39;adam&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.fit(training_images, training_labels, epochs=50, callbacks=[callbacks]) . Epoch 1/50 1875/1875 [==============================] - 2s 756us/step - loss: 0.4989 - accuracy: 0.8232 Epoch 2/50 1875/1875 [==============================] - 1s 756us/step - loss: 0.3711 - accuracy: 0.8649 Epoch 3/50 1875/1875 [==============================] - 1s 756us/step - loss: 0.3344 - accuracy: 0.8776 Epoch 4/50 1875/1875 [==============================] - 1s 749us/step - loss: 0.3114 - accuracy: 0.8853 Epoch 5/50 1875/1875 [==============================] - 1s 756us/step - loss: 0.2932 - accuracy: 0.8913 Epoch 6/50 1875/1875 [==============================] - 1s 747us/step - loss: 0.2788 - accuracy: 0.8967 Epoch 7/50 1875/1875 [==============================] - 1s 750us/step - loss: 0.2646 - accuracy: 0.9012 Epoch 8/50 1875/1875 [==============================] - 1s 761us/step - loss: 0.2573 - accuracy: 0.9042 Epoch 9/50 1875/1875 [==============================] - 1s 752us/step - loss: 0.2458 - accuracy: 0.9086 Epoch 10/50 1875/1875 [==============================] - 1s 778us/step - loss: 0.2399 - accuracy: 0.9111 Epoch 11/50 1875/1875 [==============================] - 1s 764us/step - loss: 0.2291 - accuracy: 0.9140 Epoch 12/50 1875/1875 [==============================] - 1s 759us/step - loss: 0.2235 - accuracy: 0.9167 Epoch 13/50 1875/1875 [==============================] - 1s 768us/step - loss: 0.2171 - accuracy: 0.9190 Epoch 14/50 1875/1875 [==============================] - 1s 756us/step - loss: 0.2120 - accuracy: 0.9210 Epoch 15/50 1875/1875 [==============================] - 1s 795us/step - loss: 0.2069 - accuracy: 0.9226 Epoch 16/50 1875/1875 [==============================] - 1s 778us/step - loss: 0.1991 - accuracy: 0.9255 Epoch 17/50 1875/1875 [==============================] - 1s 779us/step - loss: 0.1936 - accuracy: 0.9273 Epoch 18/50 1875/1875 [==============================] - 1s 758us/step - loss: 0.1892 - accuracy: 0.9290 Epoch 19/50 1875/1875 [==============================] - 1s 771us/step - loss: 0.1844 - accuracy: 0.9310 Epoch 20/50 1875/1875 [==============================] - 1s 762us/step - loss: 0.1793 - accuracy: 0.9328 Epoch 21/50 1875/1875 [==============================] - 1s 762us/step - loss: 0.1762 - accuracy: 0.9338 Epoch 22/50 1875/1875 [==============================] - 1s 758us/step - loss: 0.1702 - accuracy: 0.9360 Epoch 23/50 1875/1875 [==============================] - 1s 771us/step - loss: 0.1670 - accuracy: 0.9373 Epoch 24/50 1875/1875 [==============================] - 2s 800us/step - loss: 0.1628 - accuracy: 0.9391 Epoch 25/50 1875/1875 [==============================] - 1s 761us/step - loss: 0.1600 - accuracy: 0.9400 Epoch 26/50 1875/1875 [==============================] - 1s 762us/step - loss: 0.1559 - accuracy: 0.9410 Epoch 27/50 1875/1875 [==============================] - 1s 757us/step - loss: 0.1526 - accuracy: 0.9418 Epoch 28/50 1875/1875 [==============================] - 1s 768us/step - loss: 0.1479 - accuracy: 0.9440 Epoch 29/50 1875/1875 [==============================] - 1s 760us/step - loss: 0.1463 - accuracy: 0.9447 Epoch 30/50 1875/1875 [==============================] - 1s 785us/step - loss: 0.1409 - accuracy: 0.9468 Epoch 31/50 1875/1875 [==============================] - 1s 768us/step - loss: 0.1395 - accuracy: 0.9477 Epoch 32/50 1875/1875 [==============================] - 1s 798us/step - loss: 0.1371 - accuracy: 0.9486 Epoch 33/50 1875/1875 [==============================] - 1s 754us/step - loss: 0.1353 - accuracy: 0.9484 Epoch 34/50 1875/1875 [==============================] - 1s 760us/step - loss: 0.1303 - accuracy: 0.9503 Reached 95% accuracy so cancelling training! . &lt;keras.callbacks.History at 0x7ff1a1261730&gt; . Callbacks are cool! . Next chapter will use the more efficient approach of convolutions .",
            "url": "https://adrianmoses.github.io/ml-journal/2021/09/24/ai-and-ml-for-coders-ch2.html",
            "relUrl": "/2021/09/24/ai-and-ml-for-coders-ch2.html",
            "date": " • Sep 24, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "AI and ML for Coders",
            "content": "import tensorflow as tf import numpy as np from tensorflow.keras import Sequential from tensorflow.keras.layers import Dense l0 = Dense(units=1, input_shape=[1]) model = Sequential([l0]) model.compile(optimizer=&#39;sgd&#39;, loss=&#39;mean_squared_error&#39;) xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float) ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float) model.fit(xs, ys, epochs=500) print(model.predict([10.0])) print(&quot;Here&#39;s what I learned: {}&quot;.format(l0.get_weights())) .",
            "url": "https://adrianmoses.github.io/ml-journal/2021/09/24/ai-and-ml-for-coders-ch1.html",
            "relUrl": "/2021/09/24/ai-and-ml-for-coders-ch1.html",
            "date": " • Sep 24, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Mastering Spacy Ch. 1",
            "content": "import spacy . !python -m spacy download en . ⚠ As of spaCy v3.0, shortcuts like &#39;en&#39; are deprecated. Please use the full pipeline package name &#39;en_core_web_sm&#39; instead. Collecting en-core-web-sm==3.1.0 Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl (13.6 MB) |████████████████████████████████| 13.6 MB 7.4 MB/s Requirement already satisfied: spacy&lt;3.2.0,&gt;=3.1.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from en-core-web-sm==3.1.0) (3.1.2) Requirement already satisfied: typer&lt;0.4.0,&gt;=0.3.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (0.3.2) Requirement already satisfied: packaging&gt;=20.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (20.9) Requirement already satisfied: numpy&gt;=1.15.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (1.20.1) Requirement already satisfied: setuptools in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (52.0.0.post20210125) Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (3.0.5) Requirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.1 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (2.4.1) Requirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.4 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (2.0.6) Requirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.7 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (3.0.8) Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (1.0.5) Requirement already satisfied: thinc&lt;8.1.0,&gt;=8.0.8 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (8.0.10) Requirement already satisfied: pydantic!=1.8,!=1.8.1,&lt;1.9.0,&gt;=1.7.4 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (1.8.2) Requirement already satisfied: blis&lt;0.8.0,&gt;=0.4.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (0.7.4) Requirement already satisfied: pathy&gt;=0.3.5 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (0.6.0) Requirement already satisfied: jinja2 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (2.11.3) Requirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (2.25.1) Requirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (4.59.0) Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (2.0.5) Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.8.1 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (0.8.2) Requirement already satisfied: pyparsing&gt;=2.0.2 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from packaging&gt;=20.0-&gt;spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (2.4.7) Requirement already satisfied: smart-open&lt;6.0.0,&gt;=5.0.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from pathy&gt;=0.3.5-&gt;spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (5.2.1) Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,&lt;1.9.0,&gt;=1.7.4-&gt;spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (3.7.4.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (2020.12.5) Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (4.0.0) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (1.26.4) Requirement already satisfied: click&lt;7.2.0,&gt;=7.1.1 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from typer&lt;0.4.0,&gt;=0.3.0-&gt;spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (7.1.2) Requirement already satisfied: MarkupSafe&gt;=0.23 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from jinja2-&gt;spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (1.1.1) ✔ Download and installation successful You can now load the package via spacy.load(&#39;en_core_web_sm&#39;) . nlp = spacy.load(&#39;en_core_web_sm&#39;) doc = nlp(&#39;I have a ginger cat.&#39;) . doc . I have a ginger cat. . DisplaCy . Let&#39;s take a look at the POS tagging and tokens with the Dispacy graphics . from spacy import displacy doc = nlp(&#39;Bill Gates is the CEO of Microsoft&#39;) displacy.render(doc, style=&#39;dep&#39;) . Bill PROPN Gates PROPN is AUX the DET CEO NOUN of ADP Microsoft PROPN compound nsubj det attr prep pobj",
            "url": "https://adrianmoses.github.io/ml-journal/2021/09/23/mastering-spacy-chapter-1.html",
            "relUrl": "/2021/09/23/mastering-spacy-chapter-1.html",
            "date": " • Sep 23, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Intro to NLP",
            "content": "This course appears a bit more hands-on as it starts off with introducing spacy. spacy.load code examples are added . !pip install spacy !python -m spacy download en . import spacy nlp = spacy.load(&#39;en_core_web_sm&#39;) . doc = nlp(&quot;Tea is healthy and calming, don&#39;t you think?&quot;) . for token in doc: print(token) . Tea is healthy and calming , do n&#39;t you think ? . These are token objects. A token object has the lemmatization with token.lemma_ and if it&#39;s a stop word token.is_stop . print(f&quot;Token t tLemma t tStopword&quot;.format(&#39;Token&#39;, &#39;Lemma&#39;, &#39;Stopword&#39;)) print(&quot;-&quot;*40) for token in doc: print(f&quot;{str(token)} t t{token.lemma_} t t{token.is_stop}&quot;) . Token Lemma Stopword - Tea tea False is be True healthy healthy False and and True calming calm False , , False do do True n&#39;t n&#39;t True you you True think think False ? ? False . Lemmatization and Stopwords can be helpful, but also detrimental to a model&#39;s performance. Consider it as hyperparameters for tweaking the performance of a model. . Pattern Matching . Spacy has pattern matching capabilities that are easier to use then Regex . from spacy.matcher import PhraseMatcher matcher = PhraseMatcher(nlp.vocab, attr=&#39;LOWER&#39;) . Matchers depend on a vocabulary model, so the english model above was used. attr=&#39;LOWER&#39; lowers all text ensuring case insentivity . Convert the terms we need to match to documents and add to the matcher . terms = [&#39;Galaxy Note&#39;, &#39;iPhone 11&#39;, &#39;iPhone XS&#39;, &#39;Google Pixel&#39;] patterns = [nlp(text) for text in terms] matcher.add(&quot;TerminologyList&quot;, patterns) . text_doc = nlp(&quot;Glowing review overall, and some really interesting side-by-side &quot; &quot;photography tests pitting the iPhone 11 Pro against the &quot; &quot;Galaxy Note 10 Plus and last year&#39;s iPhone XS and Google Pixel 3.&quot;) matches = matcher(text_doc) print(matches) . [(3766102292120407359, 17, 19), (3766102292120407359, 22, 24), (3766102292120407359, 30, 32), (3766102292120407359, 33, 35)] . A match is a tuple of (match_id, start_pos, end_pos) . match_id, start, end = matches[0] print(nlp.vocab.strings[match_id], text_doc[start:end]) . TerminologyList iPhone 11 . Text Classification . Machines need numeric representations of text. . One way to convert a sentence or phrase to a numeric represneataiotion is to count the occurances of a word in a document Then the vector is the length of every word in the entire corpus. A variation of one-hot encoding. . This is called bag of words . Another approach is by scaling the term count by the overall term&#39;s frequency in the corpus. The name of that representation is called TF-IDF or Term Frequency - Inverse Document Frequency . spacy can support bag of words with the TextCategorizer . nlp = spacy.blank(&quot;en&quot;) textcat = nlp.create_pipe(&quot;textcat&quot;, config={ &quot;exclusive_classes&quot;: True, &quot;architecture&quot;: &quot;bow&quot; }) nlp.add_pipe(textcat) . ConfigValidationError Traceback (most recent call last) &lt;ipython-input-14-b892f9c822da&gt; in &lt;module&gt; 1 nlp = spacy.blank(&#34;en&#34;) 2 -&gt; 3 textcat = nlp.create_pipe(&#34;textcat&#34;, config={ 4 &#34;exclusive_classes&#34;: True, 5 &#34;architecture&#34;: &#34;bow&#34; ~/opt/anaconda3/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate) 659 # We&#39;re calling the internal _fill here to avoid constructing the 660 # registered functions twice --&gt; 661 resolved = registry.resolve(cfg, validate=validate) 662 filled = registry.fill({&#34;cfg&#34;: cfg[factory_name]}, validate=validate)[&#34;cfg&#34;] 663 filled = Config(filled) ~/opt/anaconda3/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate) 727 validate: bool = True, 728 ) -&gt; Dict[str, Any]: --&gt; 729 resolved, _ = cls._make( 730 config, schema=schema, overrides=overrides, validate=validate, resolve=True 731 ) ~/opt/anaconda3/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate) 776 if not is_interpolated: 777 config = Config(orig_config).interpolate() --&gt; 778 filled, _, resolved = cls._fill( 779 config, schema, validate=validate, overrides=overrides, resolve=resolve 780 ) ~/opt/anaconda3/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides) 831 schema.__fields__[key] = copy_model_field(field, Any) 832 promise_schema = cls.make_promise_schema(value, resolve=resolve) --&gt; 833 filled[key], validation[v_key], final[key] = cls._fill( 834 value, 835 promise_schema, ~/opt/anaconda3/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides) 897 result = schema.parse_obj(validation) 898 except ValidationError as e: --&gt; 899 raise ConfigValidationError( 900 config=config, errors=e.errors(), parent=parent 901 ) from None ConfigValidationError: Config validation error textcat -&gt; architecture extra fields not permitted textcat -&gt; exclusive_classes extra fields not permitted {&#39;nlp&#39;: &lt;spacy.lang.en.English object at 0x7fdf109c6fa0&gt;, &#39;name&#39;: &#39;textcat&#39;, &#39;architecture&#39;: &#39;bow&#39;, &#39;exclusive_classes&#39;: True, &#39;model&#39;: {&#39;@architectures&#39;: &#39;spacy.TextCatEnsemble.v2&#39;, &#39;linear_model&#39;: {&#39;@architectures&#39;: &#39;spacy.TextCatBOW.v2&#39;, &#39;exclusive_classes&#39;: True, &#39;ngram_size&#39;: 1, &#39;no_output_layer&#39;: False}, &#39;tok2vec&#39;: {&#39;@architectures&#39;: &#39;spacy.Tok2Vec.v2&#39;, &#39;embed&#39;: {&#39;@architectures&#39;: &#39;spacy.MultiHashEmbed.v2&#39;, &#39;width&#39;: 64, &#39;rows&#39;: [2000, 2000, 1000, 1000, 1000, 1000], &#39;attrs&#39;: [&#39;ORTH&#39;, &#39;LOWER&#39;, &#39;PREFIX&#39;, &#39;SUFFIX&#39;, &#39;SHAPE&#39;, &#39;ID&#39;], &#39;include_static_vectors&#39;: False}, &#39;encode&#39;: {&#39;@architectures&#39;: &#39;spacy.MaxoutWindowEncoder.v2&#39;, &#39;width&#39;: 64, &#39;window_size&#39;: 1, &#39;maxout_pieces&#39;: 3, &#39;depth&#39;: 2}}}, &#39;threshold&#39;: 0.5, &#39;@factories&#39;: &#39;textcat&#39;} .",
            "url": "https://adrianmoses.github.io/ml-journal/jupyter/2021/09/18/intro-to-nlp.html",
            "relUrl": "/jupyter/2021/09/18/intro-to-nlp.html",
            "date": " • Sep 18, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://adrianmoses.github.io/ml-journal/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://adrianmoses.github.io/ml-journal/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://adrianmoses.github.io/ml-journal/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}