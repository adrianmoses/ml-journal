{
  
    
        "post0": {
            "title": "AI and ML for Coders Ch 4",
            "content": "!pip install tensorflow-datasets . Requirement already satisfied: tensorflow-datasets in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (4.4.0) Requirement already satisfied: requests&gt;=2.19.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets) (2.25.1) Requirement already satisfied: absl-py in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets) (0.12.0) Requirement already satisfied: dill in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets) (0.3.4) Requirement already satisfied: termcolor in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets) (1.1.0) Requirement already satisfied: tqdm in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets) (4.59.0) Requirement already satisfied: six in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets) (1.15.0) Requirement already satisfied: future in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets) (0.18.2) Requirement already satisfied: protobuf&gt;=3.12.2 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets) (3.18.0) Requirement already satisfied: tensorflow-metadata in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets) (1.2.0) Requirement already satisfied: attrs&gt;=18.1.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets) (20.3.0) Requirement already satisfied: numpy in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets) (1.19.5) Requirement already satisfied: promise in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets) (2.3) Requirement already satisfied: importlib-resources in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets) (5.2.2) Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from requests&gt;=2.19.0-&gt;tensorflow-datasets) (4.0.0) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from requests&gt;=2.19.0-&gt;tensorflow-datasets) (1.26.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from requests&gt;=2.19.0-&gt;tensorflow-datasets) (2020.12.5) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from requests&gt;=2.19.0-&gt;tensorflow-datasets) (2.10) Requirement already satisfied: zipp&gt;=3.1.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from importlib-resources-&gt;tensorflow-datasets) (3.4.1) Requirement already satisfied: googleapis-common-protos&lt;2,&gt;=1.52.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-metadata-&gt;tensorflow-datasets) (1.53.0) . Pass the name of the dataset ro the argumane of load() . import tensorflow as tf import tensorflow_datasets as tfds mnist_data = tfds.load(&quot;fashion_mnist&quot;) for item in mnist_data: print(item) . train test . These are dictionary key strings: train and test . Let&#39;s now load the training data only . mnist_train = tfds.load(name=&quot;fashion_mnist&quot;, split=&quot;train&quot;) assert isinstance(mnist_train, tf.data.Dataset) print(type(mnist_train)) . &lt;class &#39;tensorflow.python.data.ops.dataset_ops.PrefetchDataset&#39;&gt; . for item in mnist_train.take(1): print(type(item)) print(item.keys()) print(item[&#39;image&#39;]) print(item[&#39;label&#39;]) . &lt;class &#39;dict&#39;&gt; dict_keys([&#39;image&#39;, &#39;label&#39;]) tf.Tensor( [[[ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 18] [ 77] [227] [227] [208] [210] [225] [216] [ 85] [ 32] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0]] [[ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 61] [100] [ 97] [ 80] [ 57] [117] [227] [238] [115] [ 49] [ 78] [106] [108] [ 71] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0]] [[ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 81] [105] [ 80] [ 69] [ 72] [ 64] [ 44] [ 21] [ 13] [ 44] [ 69] [ 75] [ 75] [ 80] [114] [ 80] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0]] [[ 0] [ 0] [ 0] [ 0] [ 0] [ 26] [ 92] [ 69] [ 68] [ 75] [ 75] [ 71] [ 74] [ 83] [ 75] [ 77] [ 78] [ 74] [ 74] [ 83] [ 77] [108] [ 34] [ 0] [ 0] [ 0] [ 0] [ 0]] [[ 0] [ 0] [ 0] [ 0] [ 0] [ 55] [ 92] [ 69] [ 74] [ 74] [ 71] [ 71] [ 77] [ 69] [ 66] [ 75] [ 74] [ 77] [ 80] [ 80] [ 78] [ 94] [ 63] [ 0] [ 0] [ 0] [ 0] [ 0]] [[ 0] [ 0] [ 0] [ 0] [ 0] [ 63] [ 95] [ 66] [ 68] [ 72] [ 72] [ 69] [ 72] [ 74] [ 74] [ 74] [ 75] [ 75] [ 77] [ 80] [ 77] [106] [ 61] [ 0] [ 0] [ 0] [ 0] [ 0]] [[ 0] [ 0] [ 0] [ 0] [ 0] [ 80] [108] [ 71] [ 69] [ 72] [ 71] [ 69] [ 72] [ 75] [ 75] [ 72] [ 72] [ 75] [ 78] [ 72] [ 85] [128] [ 64] [ 0] [ 0] [ 0] [ 0] [ 0]] [[ 0] [ 0] [ 0] [ 0] [ 0] [ 88] [120] [ 75] [ 74] [ 77] [ 75] [ 72] [ 77] [ 74] [ 74] [ 77] [ 78] [ 83] [ 83] [ 66] [111] [123] [ 78] [ 0] [ 0] [ 0] [ 0] [ 0]] [[ 0] [ 0] [ 0] [ 0] [ 0] [ 85] [134] [ 74] [ 85] [ 69] [ 75] [ 75] [ 74] [ 75] [ 74] [ 75] [ 75] [ 81] [ 75] [ 61] [151] [115] [ 91] [ 12] [ 0] [ 0] [ 0] [ 0]] [[ 0] [ 0] [ 0] [ 0] [ 10] [ 85] [153] [ 83] [ 80] [ 68] [ 77] [ 75] [ 74] [ 75] [ 74] [ 75] [ 77] [ 80] [ 68] [ 61] [162] [122] [ 78] [ 6] [ 0] [ 0] [ 0] [ 0]] [[ 0] [ 0] [ 0] [ 0] [ 30] [ 75] [154] [ 85] [ 80] [ 71] [ 80] [ 72] [ 77] [ 75] [ 75] [ 77] [ 78] [ 77] [ 75] [ 49] [191] [132] [ 72] [ 15] [ 0] [ 0] [ 0] [ 0]] [[ 0] [ 0] [ 0] [ 0] [ 58] [ 66] [174] [115] [ 66] [ 77] [ 80] [ 72] [ 78] [ 75] [ 77] [ 78] [ 78] [ 77] [ 66] [ 49] [222] [131] [ 77] [ 37] [ 0] [ 0] [ 0] [ 0]] [[ 0] [ 0] [ 0] [ 0] [ 69] [ 55] [179] [139] [ 55] [ 92] [ 74] [ 74] [ 78] [ 74] [ 78] [ 77] [ 75] [ 80] [ 64] [ 55] [242] [111] [ 95] [ 44] [ 0] [ 0] [ 0] [ 0]] [[ 0] [ 0] [ 0] [ 0] [ 74] [ 57] [159] [180] [ 55] [ 92] [ 64] [ 72] [ 74] [ 74] [ 77] [ 75] [ 77] [ 78] [ 55] [ 66] [255] [ 97] [108] [ 49] [ 0] [ 0] [ 0] [ 0]] [[ 0] [ 0] [ 0] [ 0] [ 74] [ 66] [145] [153] [ 72] [ 83] [ 58] [ 78] [ 77] [ 75] [ 75] [ 75] [ 72] [ 80] [ 30] [132] [255] [ 37] [122] [ 60] [ 0] [ 0] [ 0] [ 0]] [[ 0] [ 0] [ 0] [ 0] [ 80] [ 69] [142] [180] [142] [ 57] [ 64] [ 78] [ 74] [ 75] [ 75] [ 75] [ 72] [ 85] [ 21] [185] [227] [ 37] [143] [ 63] [ 0] [ 0] [ 0] [ 0]] [[ 0] [ 0] [ 0] [ 0] [ 83] [ 71] [136] [194] [126] [ 46] [ 69] [ 75] [ 72] [ 75] [ 75] [ 75] [ 74] [ 78] [ 38] [139] [185] [ 60] [151] [ 58] [ 0] [ 0] [ 0] [ 0]] [[ 0] [ 0] [ 0] [ 4] [ 81] [ 74] [145] [177] [ 78] [ 49] [ 74] [ 77] [ 75] [ 75] [ 75] [ 75] [ 74] [ 72] [ 63] [ 80] [156] [117] [153] [ 55] [ 0] [ 0] [ 0] [ 0]] [[ 0] [ 0] [ 0] [ 10] [ 80] [ 72] [157] [163] [ 61] [ 55] [ 75] [ 77] [ 75] [ 77] [ 75] [ 75] [ 75] [ 77] [ 71] [ 60] [ 98] [156] [132] [ 58] [ 0] [ 0] [ 0] [ 0]] [[ 0] [ 0] [ 0] [ 13] [ 77] [ 74] [157] [143] [ 43] [ 61] [ 72] [ 75] [ 77] [ 75] [ 74] [ 77] [ 77] [ 75] [ 71] [ 58] [ 80] [157] [120] [ 66] [ 0] [ 0] [ 0] [ 0]] [[ 0] [ 0] [ 0] [ 18] [ 81] [ 74] [156] [114] [ 35] [ 72] [ 71] [ 75] [ 78] [ 72] [ 66] [ 80] [ 78] [ 77] [ 75] [ 64] [ 63] [165] [119] [ 68] [ 0] [ 0] [ 0] [ 0]] [[ 0] [ 0] [ 0] [ 23] [ 85] [ 81] [177] [ 57] [ 52] [ 77] [ 71] [ 78] [ 80] [ 72] [ 75] [ 74] [ 77] [ 77] [ 75] [ 64] [ 37] [173] [ 95] [ 72] [ 0] [ 0] [ 0] [ 0]] [[ 0] [ 0] [ 0] [ 26] [ 81] [ 86] [160] [ 20] [ 75] [ 77] [ 77] [ 80] [ 78] [ 80] [ 89] [ 78] [ 81] [ 83] [ 80] [ 74] [ 20] [177] [ 77] [ 74] [ 0] [ 0] [ 0] [ 0]] [[ 0] [ 0] [ 0] [ 49] [ 77] [ 91] [200] [ 0] [ 83] [ 95] [ 86] [ 88] [ 88] [ 89] [ 88] [ 89] [ 88] [ 83] [ 89] [ 86] [ 0] [191] [ 78] [ 80] [ 24] [ 0] [ 0] [ 0]] [[ 0] [ 0] [ 0] [ 54] [ 71] [108] [165] [ 0] [ 24] [ 57] [ 52] [ 57] [ 60] [ 60] [ 60] [ 63] [ 63] [ 77] [ 89] [ 52] [ 0] [211] [ 97] [ 77] [ 61] [ 0] [ 0] [ 0]] [[ 0] [ 0] [ 0] [ 68] [ 91] [117] [137] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 18] [216] [ 94] [ 97] [ 57] [ 0] [ 0] [ 0]] [[ 0] [ 0] [ 0] [ 54] [115] [105] [185] [ 0] [ 0] [ 1] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [153] [ 78] [106] [ 37] [ 0] [ 0] [ 0]] [[ 0] [ 0] [ 0] [ 18] [ 61] [ 41] [103] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [ 0] [106] [ 47] [ 69] [ 23] [ 0] [ 0] [ 0]]], shape=(28, 28, 1), dtype=uint8) tf.Tensor(2, shape=(), dtype=int64) . You can also grab info about the data . mnist_test, info = tfds.load(&quot;fashion_mnist&quot;, with_info=&quot;true&quot;) print(info) . tfds.core.DatasetInfo( name=&#39;fashion_mnist&#39;, full_name=&#39;fashion_mnist/3.0.1&#39;, description=&#34;&#34;&#34; Fashion-MNIST is a dataset of Zalando&#39;s article images consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. &#34;&#34;&#34;, homepage=&#39;https://github.com/zalandoresearch/fashion-mnist&#39;, data_path=&#39;/Users/adrianmoses/tensorflow_datasets/fashion_mnist/3.0.1&#39;, download_size=29.45 MiB, dataset_size=36.42 MiB, features=FeaturesDict({ &#39;image&#39;: Image(shape=(28, 28, 1), dtype=tf.uint8), &#39;label&#39;: ClassLabel(shape=(), dtype=tf.int64, num_classes=10), }), supervised_keys=(&#39;image&#39;, &#39;label&#39;), disable_shuffling=False, splits={ &#39;test&#39;: &lt;SplitInfo num_examples=10000, num_shards=1&gt;, &#39;train&#39;: &lt;SplitInfo num_examples=60000, num_shards=1&gt;, }, citation=&#34;&#34;&#34;@article{DBLP:journals/corr/abs-1708-07747, author = {Han Xiao and Kashif Rasul and Roland Vollgraf}, title = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms}, journal = {CoRR}, volume = {abs/1708.07747}, year = {2017}, url = {http://arxiv.org/abs/1708.07747}, archivePrefix = {arXiv}, eprint = {1708.07747}, timestamp = {Mon, 13 Aug 2018 16:47:27 +0200}, biburl = {https://dblp.org/rec/bib/journals/corr/abs-1708-07747}, bibsource = {dblp computer science bibliography, https://dblp.org} }&#34;&#34;&#34;, ) . Using TFDS with Keras Models . Here&#39;s how to load in trainging and test data for feeding into keras models . Note: as_supervised ensures we get tuples of (X, y) . (training_images, training_labels), (test_images, test_labels) = tfds.as_numpy(tfds.load(&#39;fashion_mnist&#39;, split=[&#39;train&#39;,&#39;test&#39;], batch_size=-1, as_supervised=True)) . WARNING:tensorflow:From /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages/tensorflow_datasets/core/dataset_builder.py:622: get_single_element (from tensorflow.python.data.experimental.ops.get_single_element) is deprecated and will be removed in a future version. Instructions for updating: Use `tf.data.Dataset.get_single_element()`. . WARNING:tensorflow:From /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages/tensorflow_datasets/core/dataset_builder.py:622: get_single_element (from tensorflow.python.data.experimental.ops.get_single_element) is deprecated and will be removed in a future version. Instructions for updating: Use `tf.data.Dataset.get_single_element()`. . training_images.shape . (60000, 28, 28, 1) . Batch datasets with .batch() call . data = tfds.load(&#39;horses_or_humans&#39;, split=&#39;train&#39;, as_supervised=True) train_batches = data.shuffle(100).batch(10) . model = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(16, (3,3), activation=&#39;relu&#39;, input_shape=(300, 300, 3)), tf.keras.layers.MaxPooling2D(2, 2), tf.keras.layers.Conv2D(32, (3, 3), activation=&#39;relu&#39;), tf.keras.layers.MaxPooling2D(2, 2), tf.keras.layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;), tf.keras.layers.MaxPooling2D(2, 2), tf.keras.layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;), tf.keras.layers.MaxPooling2D(2, 2), tf.keras.layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;), tf.keras.layers.MaxPooling2D(2, 2), tf.keras.layers.Flatten(), tf.keras.layers.Dense(512, activation=&#39;relu&#39;), tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;), ]) model.compile(optimizer=&#39;Adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) history = model.fit(train_batches, epochs=1) . 103/103 [==============================] - 24s 225ms/step - loss: 3.0123 - accuracy: 0.8471 . Load cells by version using the semver version system. . data, info = tfds.load(&quot;cnn_dailymail:3.1.0&quot;, with_info=True) . Downloading and preparing dataset 558.32 MiB (download: 558.32 MiB, generated: 1.27 GiB, total: 1.82 GiB) to /Users/adrianmoses/tensorflow_datasets/cnn_dailymail/3.1.0... . Dl Completed...: 0 url [00:00, ? url/s] Dl Completed...: 0%| | 0/1 [00:00&lt;?, ? url/s] Dl Completed...: 0%| | 0/2 [00:00&lt;?, ? url/s] Dl Completed...: 0%| | 0/3 [00:00&lt;?, ? url/s] Dl Completed...: 0%| | 0/4 [00:00&lt;?, ? url/s] Dl Completed...: 0%| | 0/5 [00:00&lt;?, ? url/s] Dl Completed...: 0%| | 0/5 [00:00&lt;?, ? url/s] Dl Completed...: 0%| | 0/5 [00:00&lt;?, ? url/s] Dl Completed...: 0%| | 0/5 [00:00&lt;?, ? url/s] Dl Completed...: 20%|██ | 1/5 [00:00&lt;00:01, 3.02 url/s] Dl Completed...: 20%|██ | 1/5 [00:00&lt;00:01, 3.02 url/s] Dl Completed...: 20%|██ | 1/5 [00:00&lt;00:01, 3.02 url/s] Dl Completed...: 20%|██ | 1/5 [00:00&lt;00:01, 3.02 url/s] Dl Completed...: 40%|████ | 2/5 [00:00&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:01&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:02&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:03&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:04&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:05&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:06&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:06&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:06&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:06&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:06&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:06&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:06&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:06&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:06&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:06&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:06&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:06&lt;00:00, 4.67 url/s] Dl Completed...: 40%|████ | 2/5 [00:06&lt;00:00, 4.67 url/s] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:06&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:07&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:08&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:08&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:08&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:08&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:08&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:08&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:08&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:08&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:08&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:08&lt;00:05, 2.79s/ url] Dl Completed...: 60%|██████ | 3/5 [00:08&lt;00:05, 2.79s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:08&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:09&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:10&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:11&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:12&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:13&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:13&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:13&lt;00:02, 2.47s/ url] Dl Completed...: 80%|████████ | 4/5 [00:13&lt;00:02, 2.47s/ url] Dl Completed...: 100%|██████████| 5/5 [00:13&lt;00:00, 3.31s/ url] Dl Completed...: 100%|██████████| 5/5 [00:13&lt;00:00, 3.31s/ url] Dl Completed...: 100%|██████████| 5/5 [01:07&lt;00:00, 3.31s/ url] Dl Completed...: 100%|██████████| 5/5 [02:12&lt;00:00, 3.31s/ url] Extraction completed...: 100%|██████████| 2/2 [02:12&lt;00:00, 66.35s/ file] Dl Size...: 557 MiB [02:12, 4.20 MiB/s] Dl Completed...: 100%|██████████| 5/5 [02:12&lt;00:00, 26.54s/ url] . Dataset cnn_dailymail downloaded and prepared to /Users/adrianmoses/tensorflow_datasets/cnn_dailymail/3.1.0. Subsequent calls will reuse this data. . Mapping functions iwth data augmentation . data = tfds.load(&#39;horses_or_humans&#39;, split=&#39;train&#39;, as_supervised=True) . def augment_images(image, label): image = tf.cast(image, tf.float32) image = (image/255) image = tf.image.random_flip_left_right(image) return image, label . train = data.map(augment_images) . Use Tensorflow Addons to use more augmentation features . !pip install tensorflow-addons . Collecting tensorflow-addons Downloading tensorflow_addons-0.14.0-cp38-cp38-macosx_10_13_x86_64.whl (575 kB) |████████████████████████████████| 575 kB 5.1 MB/s Collecting typeguard&gt;=2.7 Downloading typeguard-2.12.1-py3-none-any.whl (17 kB) Installing collected packages: typeguard, tensorflow-addons Successfully installed tensorflow-addons-0.14.0 typeguard-2.12.1 . import tensorflow_addons as tfa def augment_images(image, label): image = tf.cast(image, tf.float32) image = (image/255) image = tf.image.random_flip_left_right(image) image = tfa.image.rotate(image, 40, interpolation=&#39;NEAREST&#39;) # rotate and fill with nearest neighboars return image, label . Using Custom Splits . Apply custom splits with the split= keyword arg . def load_cats_vs_dogs(split_string): return tfds.load(&#39;cats_vs_dogs&#39;, split=split_string, as_supervised=True) train_data = load_cats_vs_dogs(&#39;train[:80%]&#39;) validation_data = load_cats_vs_dogs(&#39;train[80%:90%]&#39;) test_data = load_cats_vs_dogs(&#39;train[-10%:]&#39;) . Dl Completed...: 0%| | 0/1 [00:00&lt;?, ? url/s] . Downloading and preparing dataset 786.68 MiB (download: 786.68 MiB, generated: Unknown size, total: 786.68 MiB) to /Users/adrianmoses/tensorflow_datasets/cats_vs_dogs/4.0.0... . Dl Size...: 100%|██████████| 786/786 [00:18&lt;00:00, 43.25 MiB/s] Dl Completed...: 100%|██████████| 1/1 [00:18&lt;00:00, 18.18s/ url] Generating splits...: 0%| | 0/1 [00:00&lt;?, ? splits/s]WARNING:absl:1738 images were corrupted and were skipped . Dataset cats_vs_dogs downloaded and prepared to /Users/adrianmoses/tensorflow_datasets/cats_vs_dogs/4.0.0. Subsequent calls will reuse this data. . Files can be loaded in parallel . file_pattern = &#39;/Users/adrianmoses/tensorflow_datasets/cats_vs_dogs/4.0.0/cats_vs_dogs-train.tfrecord*&#39; files = tf.data.Dataset.list_files(file_pattern) train_dataset = files.interleave( tf.data.TFRecordDataset, cycle_length=4, # records at a time (concurrently) num_parallel_calls=tf.data.experimental.AUTOTUNE # dynamically set based on available CPUs, use multiprocessing if not ) .",
            "url": "https://adrianmoses.github.io/ml-journal/2021/09/26/tensorflow-public-datasets.html",
            "relUrl": "/2021/09/26/tensorflow-public-datasets.html",
            "date": " • Sep 26, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "AI and ML for Coders Ch 3",
            "content": "A convolution is a filter of weights that can be multiplied by a pixel and it&#39;s neighbors to produe a new value of the pixel . For example a 3x3 grid of pixels (center pixel and surrounding 8 pixels) could be multipled by a 3x3 filter producting a new value that will replace the center pixel value . Repeate the process for every image and then a new filtered image is constructed . Certain filters will provide varying resulting images. One filter applied with negative pixels on the left, zero in the middle and positive on the right will product a filtered image with nohting but vertical lines. If the negative, zero, positive was from top to bottom then all that will be left are horizatonal lines . Information is removed based on the filters. We could ideally learn which filters reduce the image to features that are predictive for the labels. . When combined with pooling we can reduce the amount of inof n the image while maintain the features . Pooling . Pooling involves reducing an image while retaining on the core context. . *Max Pooling does this by grouping an image into smaller arrays and taking the max (highest) pixel value to replace the entire smaller array . So a 4 x 4 array split into 2 x 2 arrays would result in 4 smaller arrays. Taking the max pixel value in each array will result in 4 pixel values that now replace the 4x4 array . import tensorflow as tf data = tf.keras.datasets.fashion_mnist (training_images, training_labels), (test_images, test_labels) = data.load_data() training_images = training_images.reshape(60000, 28, 28, 1) training_images = training_images / 255.0 test_images = test_images.reshape(10000, 28, 28, 1) test_images = test_images / 255.0 model = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;), tf.keras.layers.MaxPooling2D(2, 2), tf.keras.layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;), tf.keras.layers.MaxPooling2D(2, 2), tf.keras.layers.Flatten(), tf.keras.layers.Dense(128, activation=tf.nn.relu), tf.keras.layers.Dense(10, activation=tf.nn.softmax) ]) model.compile(optimizer=&#39;adam&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.fit(training_images, training_labels, epochs=5) model.evaluate(test_images, test_labels) . Epoch 1/5 1875/1875 [==============================] - 25s 13ms/step - loss: 0.4381 - accuracy: 0.8393 Epoch 2/5 1875/1875 [==============================] - 27s 15ms/step - loss: 0.2907 - accuracy: 0.8936 Epoch 3/5 1875/1875 [==============================] - 39s 21ms/step - loss: 0.2458 - accuracy: 0.9096 Epoch 4/5 1875/1875 [==============================] - 34s 18ms/step - loss: 0.2135 - accuracy: 0.9204 Epoch 5/5 1875/1875 [==============================] - 29s 16ms/step - loss: 0.1860 - accuracy: 0.9299 313/313 [==============================] - 2s 5ms/step - loss: 0.2480 - accuracy: 0.9106 . [0.2479928731918335, 0.9106000065803528] . classifications = model.predict(test_images) print(classifications[0]) print(test_labels[0]) . [3.0219702e-08 5.2809389e-12 1.6438183e-09 3.7111588e-11 1.7116486e-10 3.4847562e-05 6.5400046e-10 1.6792927e-05 7.4534330e-11 9.9994838e-01] 9 . model.summary() . Model: &#34;sequential_2&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_4 (Conv2D) (None, 26, 26, 64) 640 _________________________________________________________________ max_pooling2d_4 (MaxPooling2 (None, 13, 13, 64) 0 _________________________________________________________________ conv2d_5 (Conv2D) (None, 11, 11, 64) 36928 _________________________________________________________________ max_pooling2d_5 (MaxPooling2 (None, 5, 5, 64) 0 _________________________________________________________________ flatten_2 (Flatten) (None, 1600) 0 _________________________________________________________________ dense_4 (Dense) (None, 128) 204928 _________________________________________________________________ dense_5 (Dense) (None, 10) 1290 ================================================================= Total params: 243,786 Trainable params: 243,786 Non-trainable params: 0 _________________________________________________________________ . Notice that the total parameters increase significantly as the image size (output shape) decreases . The convolutional and dense layers contain weights and biases as paremeters that are applied to each neuron . for the convolutonal layers there&#39;s 64 3x3 filters. The filters have 9 weights and 1 bias . So (64 * 9) + 64 = 640 parameters . The second convolution will have to consider the previous 64 filters . So (64 (64 9)) + 64 = 36928 parameters . The first dense later has to deal with a flattened 5x5 for 64 images == 1600 . That 1600 is then multiplied by the 128 neurons and added to the 128 biases . So (1600 * 128) + 128 = 204928 parameters . The final dense layer takes the output of the previous 128 and mlutiples by 10 with 10 biases . (128 * 10) + 10 = 1290 parameters . Horse or Humans Dataset . Let&#39;s try a trickier dataset. One where the images are not centered and the import objects are not the same size . We&#39;ll need to use the ImageDataGenerator to label the dataset as well . import urllib.request import zipfile url = &quot;https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip&quot; filename = &quot;horse-or-human.zip&quot; training_dir = &#39;horse-or-human/training/&#39; urllib.request.urlretrieve(url, filename) zip_ref = zipfile.ZipFile(filename, &#39;r&#39;) zip_ref.extractall(training_dir) zip_ref.close() . from tensorflow.keras.preprocessing.image import ImageDataGenerator # All iamges will be rescaled by 1./255 train_datagen = ImageDataGenerator(rescale=1/255) train_generator = train_datagen.flow_from_directory( training_dir, target_size=(300, 300), class_mode=&#39;binary&#39; ) . Found 1027 images belonging to 2 classes. . Note: Images are much larger than the MNIST dataset at 300x300 . They&#39;re also color images and so will have 3 channels in the third dimension . Also this is a binary classifier, so we will use one output neurone to produce a number between 0 and 1 . model = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(16, (3, 3), activation=&#39;relu&#39;, input_shape=(300, 300, 3)), tf.keras.layers.MaxPool2D(2, 2), tf.keras.layers.Conv2D(32, (3, 3), activation=&#39;relu&#39;), tf.keras.layers.MaxPool2D(2, 2), tf.keras.layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;), tf.keras.layers.MaxPool2D(2, 2), tf.keras.layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;), tf.keras.layers.MaxPool2D(2, 2), tf.keras.layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;), tf.keras.layers.MaxPool2D(2, 2), tf.keras.layers.Flatten(), tf.keras.layers.Dense(512, activation=&#39;relu&#39;), tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;) ]) . model.summary() . Model: &#34;sequential_3&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_7 (Conv2D) (None, 298, 298, 16) 448 _________________________________________________________________ max_pooling2d_7 (MaxPooling2 (None, 149, 149, 16) 0 _________________________________________________________________ conv2d_8 (Conv2D) (None, 147, 147, 32) 4640 _________________________________________________________________ max_pooling2d_8 (MaxPooling2 (None, 73, 73, 32) 0 _________________________________________________________________ conv2d_9 (Conv2D) (None, 71, 71, 64) 18496 _________________________________________________________________ max_pooling2d_9 (MaxPooling2 (None, 35, 35, 64) 0 _________________________________________________________________ conv2d_10 (Conv2D) (None, 33, 33, 64) 36928 _________________________________________________________________ max_pooling2d_10 (MaxPooling (None, 16, 16, 64) 0 _________________________________________________________________ conv2d_11 (Conv2D) (None, 14, 14, 64) 36928 _________________________________________________________________ max_pooling2d_11 (MaxPooling (None, 7, 7, 64) 0 _________________________________________________________________ flatten_3 (Flatten) (None, 3136) 0 _________________________________________________________________ dense_6 (Dense) (None, 512) 1606144 _________________________________________________________________ dense_7 (Dense) (None, 1) 513 ================================================================= Total params: 1,704,097 Trainable params: 1,704,097 Non-trainable params: 0 _________________________________________________________________ . model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=tf.keras.optimizers.RMSprop(lr=0.001), metrics=[&#39;accuracy&#39;] ) . /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead. warnings.warn( . !pip install pillow . Requirement already satisfied: pillow in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (8.2.0) . history = model.fit_generator(train_generator, epochs=15) . /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators. warnings.warn(&#39;`Model.fit_generator` is deprecated and &#39; . Epoch 1/15 33/33 [==============================] - 23s 654ms/step - loss: 0.5953 - accuracy: 0.6981 Epoch 2/15 33/33 [==============================] - 21s 644ms/step - loss: 0.4123 - accuracy: 0.8958 Epoch 3/15 33/33 [==============================] - 22s 655ms/step - loss: 0.2618 - accuracy: 0.9416 Epoch 4/15 33/33 [==============================] - 21s 644ms/step - loss: 0.2222 - accuracy: 0.9572 Epoch 5/15 33/33 [==============================] - 22s 666ms/step - loss: 0.0559 - accuracy: 0.9766 Epoch 6/15 33/33 [==============================] - 32s 974ms/step - loss: 0.1055 - accuracy: 0.9805 Epoch 7/15 33/33 [==============================] - 29s 864ms/step - loss: 0.1530 - accuracy: 0.9796 Epoch 8/15 33/33 [==============================] - 26s 774ms/step - loss: 0.0030 - accuracy: 0.9990 Epoch 9/15 33/33 [==============================] - 24s 702ms/step - loss: 0.4676 - accuracy: 0.9698 Epoch 10/15 33/33 [==============================] - 24s 707ms/step - loss: 0.0101 - accuracy: 0.9971 Epoch 11/15 33/33 [==============================] - 31s 936ms/step - loss: 7.2553e-04 - accuracy: 1.0000 Epoch 12/15 33/33 [==============================] - 29s 873ms/step - loss: 0.0782 - accuracy: 0.9786 Epoch 13/15 33/33 [==============================] - 23s 702ms/step - loss: 0.0145 - accuracy: 0.9942 Epoch 14/15 33/33 [==============================] - 22s 673ms/step - loss: 4.6487e-04 - accuracy: 1.0000 Epoch 15/15 33/33 [==============================] - 23s 686ms/step - loss: 0.4010 - accuracy: 0.9786 . Validation . Let&#39;s load in the validation dataset (from a separate zip) to validate the model . validation_url = &quot;https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip&quot; validation_filename = &quot;validation-horse-or-human.zip&quot; validation_dir= &quot;horse-or-human/validation&quot; urllib.request.urlretrieve(validation_url, validation_filename) zip_ref = zipfile.ZipFile(validation_filename, &#39;r&#39;) zip_ref.extractall(validation_dir) zip_ref.close() . validation_datagen = ImageDataGenerator(rescale=1/255) validation_generator = validation_datagen.flow_from_directory( validation_dir, target_size=(300, 300), class_mode=&#39;binary&#39; ) . Found 256 images belonging to 2 classes. . hist = model.fit_generator( train_generator, epochs=15, validation_data=validation_generator ) . /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators. warnings.warn(&#39;`Model.fit_generator` is deprecated and &#39; . Epoch 1/15 33/33 [==============================] - 23s 691ms/step - loss: 4.1590e-04 - accuracy: 1.0000 - val_loss: 2.0903 - val_accuracy: 0.8398 Epoch 2/15 33/33 [==============================] - 23s 690ms/step - loss: 1.2791e-04 - accuracy: 1.0000 - val_loss: 2.4805 - val_accuracy: 0.8398 Epoch 3/15 33/33 [==============================] - 23s 689ms/step - loss: 5.3452e-06 - accuracy: 1.0000 - val_loss: 3.1825 - val_accuracy: 0.8320 Epoch 4/15 33/33 [==============================] - 39s 1s/step - loss: 9.1770e-07 - accuracy: 1.0000 - val_loss: 3.7376 - val_accuracy: 0.8320 Epoch 5/15 33/33 [==============================] - 39s 1s/step - loss: 1.8552e-07 - accuracy: 1.0000 - val_loss: 3.8860 - val_accuracy: 0.8398 Epoch 6/15 33/33 [==============================] - 25s 756ms/step - loss: 2.6286e-08 - accuracy: 1.0000 - val_loss: 5.2682 - val_accuracy: 0.8164 Epoch 7/15 33/33 [==============================] - 27s 806ms/step - loss: 0.1614 - accuracy: 0.9815 - val_loss: 2.6197 - val_accuracy: 0.8086 Epoch 8/15 33/33 [==============================] - 33s 1s/step - loss: 1.6330e-04 - accuracy: 1.0000 - val_loss: 3.1222 - val_accuracy: 0.8047 Epoch 9/15 33/33 [==============================] - 31s 927ms/step - loss: 2.2588e-05 - accuracy: 1.0000 - val_loss: 3.5960 - val_accuracy: 0.8164 Epoch 10/15 33/33 [==============================] - 24s 731ms/step - loss: 3.4160e-06 - accuracy: 1.0000 - val_loss: 3.5953 - val_accuracy: 0.8320 Epoch 11/15 33/33 [==============================] - 26s 798ms/step - loss: 7.0704e-07 - accuracy: 1.0000 - val_loss: 4.2478 - val_accuracy: 0.8203 Epoch 12/15 33/33 [==============================] - 25s 757ms/step - loss: 1.1522e-07 - accuracy: 1.0000 - val_loss: 4.4623 - val_accuracy: 0.8320 Epoch 13/15 33/33 [==============================] - 25s 751ms/step - loss: 1.7506e-08 - accuracy: 1.0000 - val_loss: 4.9719 - val_accuracy: 0.8281 Epoch 14/15 33/33 [==============================] - 24s 733ms/step - loss: 0.4476 - accuracy: 0.9805 - val_loss: 2.5337 - val_accuracy: 0.7969 Epoch 15/15 33/33 [==============================] - 24s 730ms/step - loss: 0.0419 - accuracy: 0.9854 - val_loss: 3.1517 - val_accuracy: 0.7891 . Test out this model in Colab. Here&#39;s the Github Link . Image Augmentation . The images are CGI and perform decently well on real images. However it still misclassifies on images where humans or horses are positioned in ways the model hasn&#39;t seen yet. . Image augmentation can be a way to transform images in a way to better handle different use cases. with the ImageDataGenerator you can peform many taransforms . train_datagen = ImageDataGenerator( rescale=1./255, rotation_range=40, width_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode=&#39;nearest&#39; ) . This covers the common transformations . Rotation (randoming up to 40 degress left or right) | Shifting horizontally (up to 20%) | Shifting vertically (up to 20%) | Shearing (by up to 20%) | Zooming (by up to 20%) | Flipping (randomly horizontally or vertically) | Filling in any missing pixelas after a move or shear with nearest neihbors | . Transfer Learning . What if we used a large model trained on many more features and parameters and fine tuned it for our use case? . Let&#39;s try this out with Inception version 3. A large model trained on ImageNet . from tensorflow.keras.applications.inception_v3 import InceptionV3 weights_url = &quot;https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5&quot; weights_file = &quot;inception_v3.h5&quot; urllib.request.urlretrieve(weights_url, weights_file) pre_trained_model = InceptionV3(input_shape=(150, 150, 3), include_top=False, weights=None) pre_trained_model.load_weights(weights_file) . pre_trained_model.summary() . Model: &#34;inception_v3&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 150, 150, 3) 0 __________________________________________________________________________________________________ conv2d_4 (Conv2D) (None, 74, 74, 32) 864 input_1[0][0] __________________________________________________________________________________________________ batch_normalization (BatchNorma (None, 74, 74, 32) 96 conv2d_4[0][0] __________________________________________________________________________________________________ activation (Activation) (None, 74, 74, 32) 0 batch_normalization[0][0] __________________________________________________________________________________________________ conv2d_5 (Conv2D) (None, 72, 72, 32) 9216 activation[0][0] __________________________________________________________________________________________________ batch_normalization_1 (BatchNor (None, 72, 72, 32) 96 conv2d_5[0][0] __________________________________________________________________________________________________ activation_1 (Activation) (None, 72, 72, 32) 0 batch_normalization_1[0][0] __________________________________________________________________________________________________ conv2d_6 (Conv2D) (None, 72, 72, 64) 18432 activation_1[0][0] __________________________________________________________________________________________________ batch_normalization_2 (BatchNor (None, 72, 72, 64) 192 conv2d_6[0][0] __________________________________________________________________________________________________ activation_2 (Activation) (None, 72, 72, 64) 0 batch_normalization_2[0][0] __________________________________________________________________________________________________ max_pooling2d_4 (MaxPooling2D) (None, 35, 35, 64) 0 activation_2[0][0] __________________________________________________________________________________________________ conv2d_7 (Conv2D) (None, 35, 35, 80) 5120 max_pooling2d_4[0][0] __________________________________________________________________________________________________ batch_normalization_3 (BatchNor (None, 35, 35, 80) 240 conv2d_7[0][0] __________________________________________________________________________________________________ activation_3 (Activation) (None, 35, 35, 80) 0 batch_normalization_3[0][0] __________________________________________________________________________________________________ conv2d_8 (Conv2D) (None, 33, 33, 192) 138240 activation_3[0][0] __________________________________________________________________________________________________ batch_normalization_4 (BatchNor (None, 33, 33, 192) 576 conv2d_8[0][0] __________________________________________________________________________________________________ activation_4 (Activation) (None, 33, 33, 192) 0 batch_normalization_4[0][0] __________________________________________________________________________________________________ max_pooling2d_5 (MaxPooling2D) (None, 16, 16, 192) 0 activation_4[0][0] __________________________________________________________________________________________________ conv2d_12 (Conv2D) (None, 16, 16, 64) 12288 max_pooling2d_5[0][0] __________________________________________________________________________________________________ batch_normalization_8 (BatchNor (None, 16, 16, 64) 192 conv2d_12[0][0] __________________________________________________________________________________________________ activation_8 (Activation) (None, 16, 16, 64) 0 batch_normalization_8[0][0] __________________________________________________________________________________________________ conv2d_10 (Conv2D) (None, 16, 16, 48) 9216 max_pooling2d_5[0][0] __________________________________________________________________________________________________ conv2d_13 (Conv2D) (None, 16, 16, 96) 55296 activation_8[0][0] __________________________________________________________________________________________________ batch_normalization_6 (BatchNor (None, 16, 16, 48) 144 conv2d_10[0][0] __________________________________________________________________________________________________ batch_normalization_9 (BatchNor (None, 16, 16, 96) 288 conv2d_13[0][0] __________________________________________________________________________________________________ activation_6 (Activation) (None, 16, 16, 48) 0 batch_normalization_6[0][0] __________________________________________________________________________________________________ activation_9 (Activation) (None, 16, 16, 96) 0 batch_normalization_9[0][0] __________________________________________________________________________________________________ average_pooling2d (AveragePooli (None, 16, 16, 192) 0 max_pooling2d_5[0][0] __________________________________________________________________________________________________ conv2d_9 (Conv2D) (None, 16, 16, 64) 12288 max_pooling2d_5[0][0] __________________________________________________________________________________________________ conv2d_11 (Conv2D) (None, 16, 16, 64) 76800 activation_6[0][0] __________________________________________________________________________________________________ conv2d_14 (Conv2D) (None, 16, 16, 96) 82944 activation_9[0][0] __________________________________________________________________________________________________ conv2d_15 (Conv2D) (None, 16, 16, 32) 6144 average_pooling2d[0][0] __________________________________________________________________________________________________ batch_normalization_5 (BatchNor (None, 16, 16, 64) 192 conv2d_9[0][0] __________________________________________________________________________________________________ batch_normalization_7 (BatchNor (None, 16, 16, 64) 192 conv2d_11[0][0] __________________________________________________________________________________________________ batch_normalization_10 (BatchNo (None, 16, 16, 96) 288 conv2d_14[0][0] __________________________________________________________________________________________________ batch_normalization_11 (BatchNo (None, 16, 16, 32) 96 conv2d_15[0][0] __________________________________________________________________________________________________ activation_5 (Activation) (None, 16, 16, 64) 0 batch_normalization_5[0][0] __________________________________________________________________________________________________ activation_7 (Activation) (None, 16, 16, 64) 0 batch_normalization_7[0][0] __________________________________________________________________________________________________ activation_10 (Activation) (None, 16, 16, 96) 0 batch_normalization_10[0][0] __________________________________________________________________________________________________ activation_11 (Activation) (None, 16, 16, 32) 0 batch_normalization_11[0][0] __________________________________________________________________________________________________ mixed0 (Concatenate) (None, 16, 16, 256) 0 activation_5[0][0] activation_7[0][0] activation_10[0][0] activation_11[0][0] __________________________________________________________________________________________________ conv2d_19 (Conv2D) (None, 16, 16, 64) 16384 mixed0[0][0] __________________________________________________________________________________________________ batch_normalization_15 (BatchNo (None, 16, 16, 64) 192 conv2d_19[0][0] __________________________________________________________________________________________________ activation_15 (Activation) (None, 16, 16, 64) 0 batch_normalization_15[0][0] __________________________________________________________________________________________________ conv2d_17 (Conv2D) (None, 16, 16, 48) 12288 mixed0[0][0] __________________________________________________________________________________________________ conv2d_20 (Conv2D) (None, 16, 16, 96) 55296 activation_15[0][0] __________________________________________________________________________________________________ batch_normalization_13 (BatchNo (None, 16, 16, 48) 144 conv2d_17[0][0] __________________________________________________________________________________________________ batch_normalization_16 (BatchNo (None, 16, 16, 96) 288 conv2d_20[0][0] __________________________________________________________________________________________________ activation_13 (Activation) (None, 16, 16, 48) 0 batch_normalization_13[0][0] __________________________________________________________________________________________________ activation_16 (Activation) (None, 16, 16, 96) 0 batch_normalization_16[0][0] __________________________________________________________________________________________________ average_pooling2d_1 (AveragePoo (None, 16, 16, 256) 0 mixed0[0][0] __________________________________________________________________________________________________ conv2d_16 (Conv2D) (None, 16, 16, 64) 16384 mixed0[0][0] __________________________________________________________________________________________________ conv2d_18 (Conv2D) (None, 16, 16, 64) 76800 activation_13[0][0] __________________________________________________________________________________________________ conv2d_21 (Conv2D) (None, 16, 16, 96) 82944 activation_16[0][0] __________________________________________________________________________________________________ conv2d_22 (Conv2D) (None, 16, 16, 64) 16384 average_pooling2d_1[0][0] __________________________________________________________________________________________________ batch_normalization_12 (BatchNo (None, 16, 16, 64) 192 conv2d_16[0][0] __________________________________________________________________________________________________ batch_normalization_14 (BatchNo (None, 16, 16, 64) 192 conv2d_18[0][0] __________________________________________________________________________________________________ batch_normalization_17 (BatchNo (None, 16, 16, 96) 288 conv2d_21[0][0] __________________________________________________________________________________________________ batch_normalization_18 (BatchNo (None, 16, 16, 64) 192 conv2d_22[0][0] __________________________________________________________________________________________________ activation_12 (Activation) (None, 16, 16, 64) 0 batch_normalization_12[0][0] __________________________________________________________________________________________________ activation_14 (Activation) (None, 16, 16, 64) 0 batch_normalization_14[0][0] __________________________________________________________________________________________________ activation_17 (Activation) (None, 16, 16, 96) 0 batch_normalization_17[0][0] __________________________________________________________________________________________________ activation_18 (Activation) (None, 16, 16, 64) 0 batch_normalization_18[0][0] __________________________________________________________________________________________________ mixed1 (Concatenate) (None, 16, 16, 288) 0 activation_12[0][0] activation_14[0][0] activation_17[0][0] activation_18[0][0] __________________________________________________________________________________________________ conv2d_26 (Conv2D) (None, 16, 16, 64) 18432 mixed1[0][0] __________________________________________________________________________________________________ batch_normalization_22 (BatchNo (None, 16, 16, 64) 192 conv2d_26[0][0] __________________________________________________________________________________________________ activation_22 (Activation) (None, 16, 16, 64) 0 batch_normalization_22[0][0] __________________________________________________________________________________________________ conv2d_24 (Conv2D) (None, 16, 16, 48) 13824 mixed1[0][0] __________________________________________________________________________________________________ conv2d_27 (Conv2D) (None, 16, 16, 96) 55296 activation_22[0][0] __________________________________________________________________________________________________ batch_normalization_20 (BatchNo (None, 16, 16, 48) 144 conv2d_24[0][0] __________________________________________________________________________________________________ batch_normalization_23 (BatchNo (None, 16, 16, 96) 288 conv2d_27[0][0] __________________________________________________________________________________________________ activation_20 (Activation) (None, 16, 16, 48) 0 batch_normalization_20[0][0] __________________________________________________________________________________________________ activation_23 (Activation) (None, 16, 16, 96) 0 batch_normalization_23[0][0] __________________________________________________________________________________________________ average_pooling2d_2 (AveragePoo (None, 16, 16, 288) 0 mixed1[0][0] __________________________________________________________________________________________________ conv2d_23 (Conv2D) (None, 16, 16, 64) 18432 mixed1[0][0] __________________________________________________________________________________________________ conv2d_25 (Conv2D) (None, 16, 16, 64) 76800 activation_20[0][0] __________________________________________________________________________________________________ conv2d_28 (Conv2D) (None, 16, 16, 96) 82944 activation_23[0][0] __________________________________________________________________________________________________ conv2d_29 (Conv2D) (None, 16, 16, 64) 18432 average_pooling2d_2[0][0] __________________________________________________________________________________________________ batch_normalization_19 (BatchNo (None, 16, 16, 64) 192 conv2d_23[0][0] __________________________________________________________________________________________________ batch_normalization_21 (BatchNo (None, 16, 16, 64) 192 conv2d_25[0][0] __________________________________________________________________________________________________ batch_normalization_24 (BatchNo (None, 16, 16, 96) 288 conv2d_28[0][0] __________________________________________________________________________________________________ batch_normalization_25 (BatchNo (None, 16, 16, 64) 192 conv2d_29[0][0] __________________________________________________________________________________________________ activation_19 (Activation) (None, 16, 16, 64) 0 batch_normalization_19[0][0] __________________________________________________________________________________________________ activation_21 (Activation) (None, 16, 16, 64) 0 batch_normalization_21[0][0] __________________________________________________________________________________________________ activation_24 (Activation) (None, 16, 16, 96) 0 batch_normalization_24[0][0] __________________________________________________________________________________________________ activation_25 (Activation) (None, 16, 16, 64) 0 batch_normalization_25[0][0] __________________________________________________________________________________________________ mixed2 (Concatenate) (None, 16, 16, 288) 0 activation_19[0][0] activation_21[0][0] activation_24[0][0] activation_25[0][0] __________________________________________________________________________________________________ conv2d_31 (Conv2D) (None, 16, 16, 64) 18432 mixed2[0][0] __________________________________________________________________________________________________ batch_normalization_27 (BatchNo (None, 16, 16, 64) 192 conv2d_31[0][0] __________________________________________________________________________________________________ activation_27 (Activation) (None, 16, 16, 64) 0 batch_normalization_27[0][0] __________________________________________________________________________________________________ conv2d_32 (Conv2D) (None, 16, 16, 96) 55296 activation_27[0][0] __________________________________________________________________________________________________ batch_normalization_28 (BatchNo (None, 16, 16, 96) 288 conv2d_32[0][0] __________________________________________________________________________________________________ activation_28 (Activation) (None, 16, 16, 96) 0 batch_normalization_28[0][0] __________________________________________________________________________________________________ conv2d_30 (Conv2D) (None, 7, 7, 384) 995328 mixed2[0][0] __________________________________________________________________________________________________ conv2d_33 (Conv2D) (None, 7, 7, 96) 82944 activation_28[0][0] __________________________________________________________________________________________________ batch_normalization_26 (BatchNo (None, 7, 7, 384) 1152 conv2d_30[0][0] __________________________________________________________________________________________________ batch_normalization_29 (BatchNo (None, 7, 7, 96) 288 conv2d_33[0][0] __________________________________________________________________________________________________ activation_26 (Activation) (None, 7, 7, 384) 0 batch_normalization_26[0][0] __________________________________________________________________________________________________ activation_29 (Activation) (None, 7, 7, 96) 0 batch_normalization_29[0][0] __________________________________________________________________________________________________ max_pooling2d_6 (MaxPooling2D) (None, 7, 7, 288) 0 mixed2[0][0] __________________________________________________________________________________________________ mixed3 (Concatenate) (None, 7, 7, 768) 0 activation_26[0][0] activation_29[0][0] max_pooling2d_6[0][0] __________________________________________________________________________________________________ conv2d_38 (Conv2D) (None, 7, 7, 128) 98304 mixed3[0][0] __________________________________________________________________________________________________ batch_normalization_34 (BatchNo (None, 7, 7, 128) 384 conv2d_38[0][0] __________________________________________________________________________________________________ activation_34 (Activation) (None, 7, 7, 128) 0 batch_normalization_34[0][0] __________________________________________________________________________________________________ conv2d_39 (Conv2D) (None, 7, 7, 128) 114688 activation_34[0][0] __________________________________________________________________________________________________ batch_normalization_35 (BatchNo (None, 7, 7, 128) 384 conv2d_39[0][0] __________________________________________________________________________________________________ activation_35 (Activation) (None, 7, 7, 128) 0 batch_normalization_35[0][0] __________________________________________________________________________________________________ conv2d_35 (Conv2D) (None, 7, 7, 128) 98304 mixed3[0][0] __________________________________________________________________________________________________ conv2d_40 (Conv2D) (None, 7, 7, 128) 114688 activation_35[0][0] __________________________________________________________________________________________________ batch_normalization_31 (BatchNo (None, 7, 7, 128) 384 conv2d_35[0][0] __________________________________________________________________________________________________ batch_normalization_36 (BatchNo (None, 7, 7, 128) 384 conv2d_40[0][0] __________________________________________________________________________________________________ activation_31 (Activation) (None, 7, 7, 128) 0 batch_normalization_31[0][0] __________________________________________________________________________________________________ activation_36 (Activation) (None, 7, 7, 128) 0 batch_normalization_36[0][0] __________________________________________________________________________________________________ conv2d_36 (Conv2D) (None, 7, 7, 128) 114688 activation_31[0][0] __________________________________________________________________________________________________ conv2d_41 (Conv2D) (None, 7, 7, 128) 114688 activation_36[0][0] __________________________________________________________________________________________________ batch_normalization_32 (BatchNo (None, 7, 7, 128) 384 conv2d_36[0][0] __________________________________________________________________________________________________ batch_normalization_37 (BatchNo (None, 7, 7, 128) 384 conv2d_41[0][0] __________________________________________________________________________________________________ activation_32 (Activation) (None, 7, 7, 128) 0 batch_normalization_32[0][0] __________________________________________________________________________________________________ activation_37 (Activation) (None, 7, 7, 128) 0 batch_normalization_37[0][0] __________________________________________________________________________________________________ average_pooling2d_3 (AveragePoo (None, 7, 7, 768) 0 mixed3[0][0] __________________________________________________________________________________________________ conv2d_34 (Conv2D) (None, 7, 7, 192) 147456 mixed3[0][0] __________________________________________________________________________________________________ conv2d_37 (Conv2D) (None, 7, 7, 192) 172032 activation_32[0][0] __________________________________________________________________________________________________ conv2d_42 (Conv2D) (None, 7, 7, 192) 172032 activation_37[0][0] __________________________________________________________________________________________________ conv2d_43 (Conv2D) (None, 7, 7, 192) 147456 average_pooling2d_3[0][0] __________________________________________________________________________________________________ batch_normalization_30 (BatchNo (None, 7, 7, 192) 576 conv2d_34[0][0] __________________________________________________________________________________________________ batch_normalization_33 (BatchNo (None, 7, 7, 192) 576 conv2d_37[0][0] __________________________________________________________________________________________________ batch_normalization_38 (BatchNo (None, 7, 7, 192) 576 conv2d_42[0][0] __________________________________________________________________________________________________ batch_normalization_39 (BatchNo (None, 7, 7, 192) 576 conv2d_43[0][0] __________________________________________________________________________________________________ activation_30 (Activation) (None, 7, 7, 192) 0 batch_normalization_30[0][0] __________________________________________________________________________________________________ activation_33 (Activation) (None, 7, 7, 192) 0 batch_normalization_33[0][0] __________________________________________________________________________________________________ activation_38 (Activation) (None, 7, 7, 192) 0 batch_normalization_38[0][0] __________________________________________________________________________________________________ activation_39 (Activation) (None, 7, 7, 192) 0 batch_normalization_39[0][0] __________________________________________________________________________________________________ mixed4 (Concatenate) (None, 7, 7, 768) 0 activation_30[0][0] activation_33[0][0] activation_38[0][0] activation_39[0][0] __________________________________________________________________________________________________ conv2d_48 (Conv2D) (None, 7, 7, 160) 122880 mixed4[0][0] __________________________________________________________________________________________________ batch_normalization_44 (BatchNo (None, 7, 7, 160) 480 conv2d_48[0][0] __________________________________________________________________________________________________ activation_44 (Activation) (None, 7, 7, 160) 0 batch_normalization_44[0][0] __________________________________________________________________________________________________ conv2d_49 (Conv2D) (None, 7, 7, 160) 179200 activation_44[0][0] __________________________________________________________________________________________________ batch_normalization_45 (BatchNo (None, 7, 7, 160) 480 conv2d_49[0][0] __________________________________________________________________________________________________ activation_45 (Activation) (None, 7, 7, 160) 0 batch_normalization_45[0][0] __________________________________________________________________________________________________ conv2d_45 (Conv2D) (None, 7, 7, 160) 122880 mixed4[0][0] __________________________________________________________________________________________________ conv2d_50 (Conv2D) (None, 7, 7, 160) 179200 activation_45[0][0] __________________________________________________________________________________________________ batch_normalization_41 (BatchNo (None, 7, 7, 160) 480 conv2d_45[0][0] __________________________________________________________________________________________________ batch_normalization_46 (BatchNo (None, 7, 7, 160) 480 conv2d_50[0][0] __________________________________________________________________________________________________ activation_41 (Activation) (None, 7, 7, 160) 0 batch_normalization_41[0][0] __________________________________________________________________________________________________ activation_46 (Activation) (None, 7, 7, 160) 0 batch_normalization_46[0][0] __________________________________________________________________________________________________ conv2d_46 (Conv2D) (None, 7, 7, 160) 179200 activation_41[0][0] __________________________________________________________________________________________________ conv2d_51 (Conv2D) (None, 7, 7, 160) 179200 activation_46[0][0] __________________________________________________________________________________________________ batch_normalization_42 (BatchNo (None, 7, 7, 160) 480 conv2d_46[0][0] __________________________________________________________________________________________________ batch_normalization_47 (BatchNo (None, 7, 7, 160) 480 conv2d_51[0][0] __________________________________________________________________________________________________ activation_42 (Activation) (None, 7, 7, 160) 0 batch_normalization_42[0][0] __________________________________________________________________________________________________ activation_47 (Activation) (None, 7, 7, 160) 0 batch_normalization_47[0][0] __________________________________________________________________________________________________ average_pooling2d_4 (AveragePoo (None, 7, 7, 768) 0 mixed4[0][0] __________________________________________________________________________________________________ conv2d_44 (Conv2D) (None, 7, 7, 192) 147456 mixed4[0][0] __________________________________________________________________________________________________ conv2d_47 (Conv2D) (None, 7, 7, 192) 215040 activation_42[0][0] __________________________________________________________________________________________________ conv2d_52 (Conv2D) (None, 7, 7, 192) 215040 activation_47[0][0] __________________________________________________________________________________________________ conv2d_53 (Conv2D) (None, 7, 7, 192) 147456 average_pooling2d_4[0][0] __________________________________________________________________________________________________ batch_normalization_40 (BatchNo (None, 7, 7, 192) 576 conv2d_44[0][0] __________________________________________________________________________________________________ batch_normalization_43 (BatchNo (None, 7, 7, 192) 576 conv2d_47[0][0] __________________________________________________________________________________________________ batch_normalization_48 (BatchNo (None, 7, 7, 192) 576 conv2d_52[0][0] __________________________________________________________________________________________________ batch_normalization_49 (BatchNo (None, 7, 7, 192) 576 conv2d_53[0][0] __________________________________________________________________________________________________ activation_40 (Activation) (None, 7, 7, 192) 0 batch_normalization_40[0][0] __________________________________________________________________________________________________ activation_43 (Activation) (None, 7, 7, 192) 0 batch_normalization_43[0][0] __________________________________________________________________________________________________ activation_48 (Activation) (None, 7, 7, 192) 0 batch_normalization_48[0][0] __________________________________________________________________________________________________ activation_49 (Activation) (None, 7, 7, 192) 0 batch_normalization_49[0][0] __________________________________________________________________________________________________ mixed5 (Concatenate) (None, 7, 7, 768) 0 activation_40[0][0] activation_43[0][0] activation_48[0][0] activation_49[0][0] __________________________________________________________________________________________________ conv2d_58 (Conv2D) (None, 7, 7, 160) 122880 mixed5[0][0] __________________________________________________________________________________________________ batch_normalization_54 (BatchNo (None, 7, 7, 160) 480 conv2d_58[0][0] __________________________________________________________________________________________________ activation_54 (Activation) (None, 7, 7, 160) 0 batch_normalization_54[0][0] __________________________________________________________________________________________________ conv2d_59 (Conv2D) (None, 7, 7, 160) 179200 activation_54[0][0] __________________________________________________________________________________________________ batch_normalization_55 (BatchNo (None, 7, 7, 160) 480 conv2d_59[0][0] __________________________________________________________________________________________________ activation_55 (Activation) (None, 7, 7, 160) 0 batch_normalization_55[0][0] __________________________________________________________________________________________________ conv2d_55 (Conv2D) (None, 7, 7, 160) 122880 mixed5[0][0] __________________________________________________________________________________________________ conv2d_60 (Conv2D) (None, 7, 7, 160) 179200 activation_55[0][0] __________________________________________________________________________________________________ batch_normalization_51 (BatchNo (None, 7, 7, 160) 480 conv2d_55[0][0] __________________________________________________________________________________________________ batch_normalization_56 (BatchNo (None, 7, 7, 160) 480 conv2d_60[0][0] __________________________________________________________________________________________________ activation_51 (Activation) (None, 7, 7, 160) 0 batch_normalization_51[0][0] __________________________________________________________________________________________________ activation_56 (Activation) (None, 7, 7, 160) 0 batch_normalization_56[0][0] __________________________________________________________________________________________________ conv2d_56 (Conv2D) (None, 7, 7, 160) 179200 activation_51[0][0] __________________________________________________________________________________________________ conv2d_61 (Conv2D) (None, 7, 7, 160) 179200 activation_56[0][0] __________________________________________________________________________________________________ batch_normalization_52 (BatchNo (None, 7, 7, 160) 480 conv2d_56[0][0] __________________________________________________________________________________________________ batch_normalization_57 (BatchNo (None, 7, 7, 160) 480 conv2d_61[0][0] __________________________________________________________________________________________________ activation_52 (Activation) (None, 7, 7, 160) 0 batch_normalization_52[0][0] __________________________________________________________________________________________________ activation_57 (Activation) (None, 7, 7, 160) 0 batch_normalization_57[0][0] __________________________________________________________________________________________________ average_pooling2d_5 (AveragePoo (None, 7, 7, 768) 0 mixed5[0][0] __________________________________________________________________________________________________ conv2d_54 (Conv2D) (None, 7, 7, 192) 147456 mixed5[0][0] __________________________________________________________________________________________________ conv2d_57 (Conv2D) (None, 7, 7, 192) 215040 activation_52[0][0] __________________________________________________________________________________________________ conv2d_62 (Conv2D) (None, 7, 7, 192) 215040 activation_57[0][0] __________________________________________________________________________________________________ conv2d_63 (Conv2D) (None, 7, 7, 192) 147456 average_pooling2d_5[0][0] __________________________________________________________________________________________________ batch_normalization_50 (BatchNo (None, 7, 7, 192) 576 conv2d_54[0][0] __________________________________________________________________________________________________ batch_normalization_53 (BatchNo (None, 7, 7, 192) 576 conv2d_57[0][0] __________________________________________________________________________________________________ batch_normalization_58 (BatchNo (None, 7, 7, 192) 576 conv2d_62[0][0] __________________________________________________________________________________________________ batch_normalization_59 (BatchNo (None, 7, 7, 192) 576 conv2d_63[0][0] __________________________________________________________________________________________________ activation_50 (Activation) (None, 7, 7, 192) 0 batch_normalization_50[0][0] __________________________________________________________________________________________________ activation_53 (Activation) (None, 7, 7, 192) 0 batch_normalization_53[0][0] __________________________________________________________________________________________________ activation_58 (Activation) (None, 7, 7, 192) 0 batch_normalization_58[0][0] __________________________________________________________________________________________________ activation_59 (Activation) (None, 7, 7, 192) 0 batch_normalization_59[0][0] __________________________________________________________________________________________________ mixed6 (Concatenate) (None, 7, 7, 768) 0 activation_50[0][0] activation_53[0][0] activation_58[0][0] activation_59[0][0] __________________________________________________________________________________________________ conv2d_68 (Conv2D) (None, 7, 7, 192) 147456 mixed6[0][0] __________________________________________________________________________________________________ batch_normalization_64 (BatchNo (None, 7, 7, 192) 576 conv2d_68[0][0] __________________________________________________________________________________________________ activation_64 (Activation) (None, 7, 7, 192) 0 batch_normalization_64[0][0] __________________________________________________________________________________________________ conv2d_69 (Conv2D) (None, 7, 7, 192) 258048 activation_64[0][0] __________________________________________________________________________________________________ batch_normalization_65 (BatchNo (None, 7, 7, 192) 576 conv2d_69[0][0] __________________________________________________________________________________________________ activation_65 (Activation) (None, 7, 7, 192) 0 batch_normalization_65[0][0] __________________________________________________________________________________________________ conv2d_65 (Conv2D) (None, 7, 7, 192) 147456 mixed6[0][0] __________________________________________________________________________________________________ conv2d_70 (Conv2D) (None, 7, 7, 192) 258048 activation_65[0][0] __________________________________________________________________________________________________ batch_normalization_61 (BatchNo (None, 7, 7, 192) 576 conv2d_65[0][0] __________________________________________________________________________________________________ batch_normalization_66 (BatchNo (None, 7, 7, 192) 576 conv2d_70[0][0] __________________________________________________________________________________________________ activation_61 (Activation) (None, 7, 7, 192) 0 batch_normalization_61[0][0] __________________________________________________________________________________________________ activation_66 (Activation) (None, 7, 7, 192) 0 batch_normalization_66[0][0] __________________________________________________________________________________________________ conv2d_66 (Conv2D) (None, 7, 7, 192) 258048 activation_61[0][0] __________________________________________________________________________________________________ conv2d_71 (Conv2D) (None, 7, 7, 192) 258048 activation_66[0][0] __________________________________________________________________________________________________ batch_normalization_62 (BatchNo (None, 7, 7, 192) 576 conv2d_66[0][0] __________________________________________________________________________________________________ batch_normalization_67 (BatchNo (None, 7, 7, 192) 576 conv2d_71[0][0] __________________________________________________________________________________________________ activation_62 (Activation) (None, 7, 7, 192) 0 batch_normalization_62[0][0] __________________________________________________________________________________________________ activation_67 (Activation) (None, 7, 7, 192) 0 batch_normalization_67[0][0] __________________________________________________________________________________________________ average_pooling2d_6 (AveragePoo (None, 7, 7, 768) 0 mixed6[0][0] __________________________________________________________________________________________________ conv2d_64 (Conv2D) (None, 7, 7, 192) 147456 mixed6[0][0] __________________________________________________________________________________________________ conv2d_67 (Conv2D) (None, 7, 7, 192) 258048 activation_62[0][0] __________________________________________________________________________________________________ conv2d_72 (Conv2D) (None, 7, 7, 192) 258048 activation_67[0][0] __________________________________________________________________________________________________ conv2d_73 (Conv2D) (None, 7, 7, 192) 147456 average_pooling2d_6[0][0] __________________________________________________________________________________________________ batch_normalization_60 (BatchNo (None, 7, 7, 192) 576 conv2d_64[0][0] __________________________________________________________________________________________________ batch_normalization_63 (BatchNo (None, 7, 7, 192) 576 conv2d_67[0][0] __________________________________________________________________________________________________ batch_normalization_68 (BatchNo (None, 7, 7, 192) 576 conv2d_72[0][0] __________________________________________________________________________________________________ batch_normalization_69 (BatchNo (None, 7, 7, 192) 576 conv2d_73[0][0] __________________________________________________________________________________________________ activation_60 (Activation) (None, 7, 7, 192) 0 batch_normalization_60[0][0] __________________________________________________________________________________________________ activation_63 (Activation) (None, 7, 7, 192) 0 batch_normalization_63[0][0] __________________________________________________________________________________________________ activation_68 (Activation) (None, 7, 7, 192) 0 batch_normalization_68[0][0] __________________________________________________________________________________________________ activation_69 (Activation) (None, 7, 7, 192) 0 batch_normalization_69[0][0] __________________________________________________________________________________________________ mixed7 (Concatenate) (None, 7, 7, 768) 0 activation_60[0][0] activation_63[0][0] activation_68[0][0] activation_69[0][0] __________________________________________________________________________________________________ conv2d_76 (Conv2D) (None, 7, 7, 192) 147456 mixed7[0][0] __________________________________________________________________________________________________ batch_normalization_72 (BatchNo (None, 7, 7, 192) 576 conv2d_76[0][0] __________________________________________________________________________________________________ activation_72 (Activation) (None, 7, 7, 192) 0 batch_normalization_72[0][0] __________________________________________________________________________________________________ conv2d_77 (Conv2D) (None, 7, 7, 192) 258048 activation_72[0][0] __________________________________________________________________________________________________ batch_normalization_73 (BatchNo (None, 7, 7, 192) 576 conv2d_77[0][0] __________________________________________________________________________________________________ activation_73 (Activation) (None, 7, 7, 192) 0 batch_normalization_73[0][0] __________________________________________________________________________________________________ conv2d_74 (Conv2D) (None, 7, 7, 192) 147456 mixed7[0][0] __________________________________________________________________________________________________ conv2d_78 (Conv2D) (None, 7, 7, 192) 258048 activation_73[0][0] __________________________________________________________________________________________________ batch_normalization_70 (BatchNo (None, 7, 7, 192) 576 conv2d_74[0][0] __________________________________________________________________________________________________ batch_normalization_74 (BatchNo (None, 7, 7, 192) 576 conv2d_78[0][0] __________________________________________________________________________________________________ activation_70 (Activation) (None, 7, 7, 192) 0 batch_normalization_70[0][0] __________________________________________________________________________________________________ activation_74 (Activation) (None, 7, 7, 192) 0 batch_normalization_74[0][0] __________________________________________________________________________________________________ conv2d_75 (Conv2D) (None, 3, 3, 320) 552960 activation_70[0][0] __________________________________________________________________________________________________ conv2d_79 (Conv2D) (None, 3, 3, 192) 331776 activation_74[0][0] __________________________________________________________________________________________________ batch_normalization_71 (BatchNo (None, 3, 3, 320) 960 conv2d_75[0][0] __________________________________________________________________________________________________ batch_normalization_75 (BatchNo (None, 3, 3, 192) 576 conv2d_79[0][0] __________________________________________________________________________________________________ activation_71 (Activation) (None, 3, 3, 320) 0 batch_normalization_71[0][0] __________________________________________________________________________________________________ activation_75 (Activation) (None, 3, 3, 192) 0 batch_normalization_75[0][0] __________________________________________________________________________________________________ max_pooling2d_7 (MaxPooling2D) (None, 3, 3, 768) 0 mixed7[0][0] __________________________________________________________________________________________________ mixed8 (Concatenate) (None, 3, 3, 1280) 0 activation_71[0][0] activation_75[0][0] max_pooling2d_7[0][0] __________________________________________________________________________________________________ conv2d_84 (Conv2D) (None, 3, 3, 448) 573440 mixed8[0][0] __________________________________________________________________________________________________ batch_normalization_80 (BatchNo (None, 3, 3, 448) 1344 conv2d_84[0][0] __________________________________________________________________________________________________ activation_80 (Activation) (None, 3, 3, 448) 0 batch_normalization_80[0][0] __________________________________________________________________________________________________ conv2d_81 (Conv2D) (None, 3, 3, 384) 491520 mixed8[0][0] __________________________________________________________________________________________________ conv2d_85 (Conv2D) (None, 3, 3, 384) 1548288 activation_80[0][0] __________________________________________________________________________________________________ batch_normalization_77 (BatchNo (None, 3, 3, 384) 1152 conv2d_81[0][0] __________________________________________________________________________________________________ batch_normalization_81 (BatchNo (None, 3, 3, 384) 1152 conv2d_85[0][0] __________________________________________________________________________________________________ activation_77 (Activation) (None, 3, 3, 384) 0 batch_normalization_77[0][0] __________________________________________________________________________________________________ activation_81 (Activation) (None, 3, 3, 384) 0 batch_normalization_81[0][0] __________________________________________________________________________________________________ conv2d_82 (Conv2D) (None, 3, 3, 384) 442368 activation_77[0][0] __________________________________________________________________________________________________ conv2d_83 (Conv2D) (None, 3, 3, 384) 442368 activation_77[0][0] __________________________________________________________________________________________________ conv2d_86 (Conv2D) (None, 3, 3, 384) 442368 activation_81[0][0] __________________________________________________________________________________________________ conv2d_87 (Conv2D) (None, 3, 3, 384) 442368 activation_81[0][0] __________________________________________________________________________________________________ average_pooling2d_7 (AveragePoo (None, 3, 3, 1280) 0 mixed8[0][0] __________________________________________________________________________________________________ conv2d_80 (Conv2D) (None, 3, 3, 320) 409600 mixed8[0][0] __________________________________________________________________________________________________ batch_normalization_78 (BatchNo (None, 3, 3, 384) 1152 conv2d_82[0][0] __________________________________________________________________________________________________ batch_normalization_79 (BatchNo (None, 3, 3, 384) 1152 conv2d_83[0][0] __________________________________________________________________________________________________ batch_normalization_82 (BatchNo (None, 3, 3, 384) 1152 conv2d_86[0][0] __________________________________________________________________________________________________ batch_normalization_83 (BatchNo (None, 3, 3, 384) 1152 conv2d_87[0][0] __________________________________________________________________________________________________ conv2d_88 (Conv2D) (None, 3, 3, 192) 245760 average_pooling2d_7[0][0] __________________________________________________________________________________________________ batch_normalization_76 (BatchNo (None, 3, 3, 320) 960 conv2d_80[0][0] __________________________________________________________________________________________________ activation_78 (Activation) (None, 3, 3, 384) 0 batch_normalization_78[0][0] __________________________________________________________________________________________________ activation_79 (Activation) (None, 3, 3, 384) 0 batch_normalization_79[0][0] __________________________________________________________________________________________________ activation_82 (Activation) (None, 3, 3, 384) 0 batch_normalization_82[0][0] __________________________________________________________________________________________________ activation_83 (Activation) (None, 3, 3, 384) 0 batch_normalization_83[0][0] __________________________________________________________________________________________________ batch_normalization_84 (BatchNo (None, 3, 3, 192) 576 conv2d_88[0][0] __________________________________________________________________________________________________ activation_76 (Activation) (None, 3, 3, 320) 0 batch_normalization_76[0][0] __________________________________________________________________________________________________ mixed9_0 (Concatenate) (None, 3, 3, 768) 0 activation_78[0][0] activation_79[0][0] __________________________________________________________________________________________________ concatenate (Concatenate) (None, 3, 3, 768) 0 activation_82[0][0] activation_83[0][0] __________________________________________________________________________________________________ activation_84 (Activation) (None, 3, 3, 192) 0 batch_normalization_84[0][0] __________________________________________________________________________________________________ mixed9 (Concatenate) (None, 3, 3, 2048) 0 activation_76[0][0] mixed9_0[0][0] concatenate[0][0] activation_84[0][0] __________________________________________________________________________________________________ conv2d_93 (Conv2D) (None, 3, 3, 448) 917504 mixed9[0][0] __________________________________________________________________________________________________ batch_normalization_89 (BatchNo (None, 3, 3, 448) 1344 conv2d_93[0][0] __________________________________________________________________________________________________ activation_89 (Activation) (None, 3, 3, 448) 0 batch_normalization_89[0][0] __________________________________________________________________________________________________ conv2d_90 (Conv2D) (None, 3, 3, 384) 786432 mixed9[0][0] __________________________________________________________________________________________________ conv2d_94 (Conv2D) (None, 3, 3, 384) 1548288 activation_89[0][0] __________________________________________________________________________________________________ batch_normalization_86 (BatchNo (None, 3, 3, 384) 1152 conv2d_90[0][0] __________________________________________________________________________________________________ batch_normalization_90 (BatchNo (None, 3, 3, 384) 1152 conv2d_94[0][0] __________________________________________________________________________________________________ activation_86 (Activation) (None, 3, 3, 384) 0 batch_normalization_86[0][0] __________________________________________________________________________________________________ activation_90 (Activation) (None, 3, 3, 384) 0 batch_normalization_90[0][0] __________________________________________________________________________________________________ conv2d_91 (Conv2D) (None, 3, 3, 384) 442368 activation_86[0][0] __________________________________________________________________________________________________ conv2d_92 (Conv2D) (None, 3, 3, 384) 442368 activation_86[0][0] __________________________________________________________________________________________________ conv2d_95 (Conv2D) (None, 3, 3, 384) 442368 activation_90[0][0] __________________________________________________________________________________________________ conv2d_96 (Conv2D) (None, 3, 3, 384) 442368 activation_90[0][0] __________________________________________________________________________________________________ average_pooling2d_8 (AveragePoo (None, 3, 3, 2048) 0 mixed9[0][0] __________________________________________________________________________________________________ conv2d_89 (Conv2D) (None, 3, 3, 320) 655360 mixed9[0][0] __________________________________________________________________________________________________ batch_normalization_87 (BatchNo (None, 3, 3, 384) 1152 conv2d_91[0][0] __________________________________________________________________________________________________ batch_normalization_88 (BatchNo (None, 3, 3, 384) 1152 conv2d_92[0][0] __________________________________________________________________________________________________ batch_normalization_91 (BatchNo (None, 3, 3, 384) 1152 conv2d_95[0][0] __________________________________________________________________________________________________ batch_normalization_92 (BatchNo (None, 3, 3, 384) 1152 conv2d_96[0][0] __________________________________________________________________________________________________ conv2d_97 (Conv2D) (None, 3, 3, 192) 393216 average_pooling2d_8[0][0] __________________________________________________________________________________________________ batch_normalization_85 (BatchNo (None, 3, 3, 320) 960 conv2d_89[0][0] __________________________________________________________________________________________________ activation_87 (Activation) (None, 3, 3, 384) 0 batch_normalization_87[0][0] __________________________________________________________________________________________________ activation_88 (Activation) (None, 3, 3, 384) 0 batch_normalization_88[0][0] __________________________________________________________________________________________________ activation_91 (Activation) (None, 3, 3, 384) 0 batch_normalization_91[0][0] __________________________________________________________________________________________________ activation_92 (Activation) (None, 3, 3, 384) 0 batch_normalization_92[0][0] __________________________________________________________________________________________________ batch_normalization_93 (BatchNo (None, 3, 3, 192) 576 conv2d_97[0][0] __________________________________________________________________________________________________ activation_85 (Activation) (None, 3, 3, 320) 0 batch_normalization_85[0][0] __________________________________________________________________________________________________ mixed9_1 (Concatenate) (None, 3, 3, 768) 0 activation_87[0][0] activation_88[0][0] __________________________________________________________________________________________________ concatenate_1 (Concatenate) (None, 3, 3, 768) 0 activation_91[0][0] activation_92[0][0] __________________________________________________________________________________________________ activation_93 (Activation) (None, 3, 3, 192) 0 batch_normalization_93[0][0] __________________________________________________________________________________________________ mixed10 (Concatenate) (None, 3, 3, 2048) 0 activation_85[0][0] mixed9_1[0][0] concatenate_1[0][0] activation_93[0][0] ================================================================================================== Total params: 21,802,784 Trainable params: 21,768,352 Non-trainable params: 34,432 __________________________________________________________________________________________________ . for layer in pre_trained_model.layers: layer.trainable = False last_layer = pre_trained_model.get_layer(&#39;mixed7&#39;) print(&#39;last layour output shape: &#39;, last_layer.output_shape) last_output = last_layer.output . last layour output shape: (None, 7, 7, 768) . # Flatten the output layter to 1 dimension x = tf.keras.layers.Flatten()(last_output) # Add a fully connected layer with 1,024 hidden units and ReLU activation x = tf.keras.layers.Dense(1024, activation=&#39;relu&#39;)(x) # Add a final sigmoid layer for classifcation x = tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;)(x) . model = tf.keras.Model(pre_trained_model.input, x) model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001), loss=&#39;binary_crossentropy&#39;, metrics=[&#39;acc&#39;]) . Multiclass Classification . Let&#39;s see how to setup multilabel classification with categorical loss . First let&#39;s load the image dataset, buld a model and fit it . !wget --no-check-certificate https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip -O /tmp/rps.zip . --2021-09-25 15:34:34-- https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.165.144, 142.250.65.208, 142.250.64.112, ... Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.165.144|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 200682221 (191M) [application/zip] Saving to: ‘/tmp/rps.zip’ /tmp/rps.zip 100%[===================&gt;] 191.38M 47.0MB/s in 4.2s 2021-09-25 15:34:39 (45.4 MB/s) - ‘/tmp/rps.zip’ saved [200682221/200682221] . local_zip = &#39;/tmp/rps.zip&#39; zip_ref = zipfile.ZipFile(local_zip, &#39;r&#39;) zip_ref.extractall(&#39;/tmp/&#39;) zip_ref.close() TRAINING_DIR = &quot;/tmp/rps/&quot; training_datagen = ImageDataGenerator( rescale=1./255, rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode=&#39;nearest&#39; ) . train_generator = training_datagen.flow_from_directory(TRAINING_DIR, target_size=(150, 150), class_mode=&#39;categorical&#39;) . Found 2520 images belonging to 3 classes. . model = tf.keras.models.Sequential([ # first convolution tf.keras.layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;, input_shape=(150, 150, 3)), tf.keras.layers.MaxPooling2D(2, 2), # second convolution tf.keras.layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;), tf.keras.layers.MaxPooling2D(2, 2), # third convolution tf.keras.layers.Conv2D(128, (3, 3), activation=&#39;relu&#39;), tf.keras.layers.MaxPooling2D(2, 2), # fourth tf.keras.layers.Conv2D(128, (3, 3), activation=&#39;relu&#39;), tf.keras.layers.MaxPooling2D(2, 2), # flatten resoults for DNN tf.keras.layers.Flatten(), # 512 neuron hidden layer tf.keras.layers.Dense(512, activation=&#39;relu&#39;), tf.keras.layers.Dense(3, activation=&#39;softmax&#39;) ]) . model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;rmsprop&#39;, metrics=[&#39;accuracy&#39;]) . history = model.fit(train_generator, epochs=25, verbose=1) . Epoch 1/25 79/79 [==============================] - 41s 505ms/step - loss: 1.2844 - accuracy: 0.3984 Epoch 2/25 58/79 [=====================&gt;........] - ETA: 22s - loss: 0.8673 - accuracy: 0.6153 . KeyboardInterrupt Traceback (most recent call last) &lt;ipython-input-22-f84aeed72e36&gt; in &lt;module&gt; -&gt; 1 history = model.fit(train_generator, epochs=25, verbose=1) ~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing) 1182 _r=1): 1183 callbacks.on_train_batch_begin(step) -&gt; 1184 tmp_logs = self.train_function(iterator) 1185 if data_handler.should_sync: 1186 context.async_wait() ~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds) 883 884 with OptionalXlaContext(self._jit_compile): --&gt; 885 result = self._call(*args, **kwds) 886 887 new_tracing_count = self.experimental_get_tracing_count() ~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds) 915 # In this case we have created variables on the first call, so we run the 916 # defunned version which is guaranteed to never create variables. --&gt; 917 return self._stateless_fn(*args, **kwds) # pylint: disable=not-callable 918 elif self._stateful_fn is not None: 919 # Release the lock early so that multiple threads can perform the call ~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs) 3037 (graph_function, 3038 filtered_flat_args) = self._maybe_define_function(args, kwargs) -&gt; 3039 return graph_function._call_flat( 3040 filtered_flat_args, captured_inputs=graph_function.captured_inputs) # pylint: disable=protected-access 3041 ~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager) 1961 and executing_eagerly): 1962 # No tape is watching; skip to running the function. -&gt; 1963 return self._build_call_outputs(self._inference_function.call( 1964 ctx, args, cancellation_manager=cancellation_manager)) 1965 forward_backward = self._select_forward_and_backward_functions( ~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager) 589 with _InterpolateFunctionError(self): 590 if cancellation_manager is None: --&gt; 591 outputs = execute.execute( 592 str(self.signature.name), 593 num_outputs=self._num_outputs, ~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name) 57 try: 58 ctx.ensure_initialized() &gt; 59 tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name, 60 inputs, attrs, num_outputs) 61 except core._NotOkStatusException as e: KeyboardInterrupt: . Dropout Regularization . Overfitting can happen often when training NNs for many epocs . THis is because neurons can become specialized and eventually the entire network becomes specialized as the weights and biases are shared to across neurons in hidden layers . Removing a random number of neurons while training can prevent neournes from sharing weights and biases and therefore becoming overspecialzied. This is called dropout and helps to improve generatlization . tf.keras.layers.Dropout(0.2) .",
            "url": "https://adrianmoses.github.io/ml-journal/2021/09/25/ai-and-ml-for-coders-ch3.html",
            "relUrl": "/2021/09/25/ai-and-ml-for-coders-ch3.html",
            "date": " • Sep 25, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "AI and ML for Coders Ch 2",
            "content": "Introduction to Computer Vision . The ability to algorithmicly see an image as a type of clothing is very difficult to define with rule-based programming. Instead, let&#39;s use machine learning. . Using the Fashion MNIST dataset we have 10 types of images based on clothing types. Each image is 28x28 and is in black and white. Meaning each pixel value is between 0 and 255. . We can&#39;t model a linear relationship between the X (image pixels) and the Y (clothing type) . However we can use the output node layer as a represenation of the 10 clothing types. Each image will be loaded into every node and the output will spit out a probability that the image is of the clothing type the output node represents. . !pip install tensorflow-cpu . Collecting tensorflow-cpu Downloading tensorflow_cpu-2.6.0-cp38-cp38-macosx_10_11_x86_64.whl (199.0 MB) |████████████████████████████████| 199.0 MB 105 kB/s Requirement already satisfied: typing-extensions~=3.7.4 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-cpu) (3.7.4.3) Collecting keras~=2.6 Using cached keras-2.6.0-py2.py3-none-any.whl (1.3 MB) Collecting astunparse~=1.6.3 Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB) Collecting termcolor~=1.1.0 Using cached termcolor-1.1.0.tar.gz (3.9 kB) Collecting gast==0.4.0 Using cached gast-0.4.0-py3-none-any.whl (9.8 kB) Collecting google-pasta~=0.2 Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB) Requirement already satisfied: wheel~=0.35 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-cpu) (0.36.2) Collecting h5py~=3.1.0 Downloading h5py-3.1.0-cp38-cp38-macosx_10_9_x86_64.whl (2.9 MB) |████████████████████████████████| 2.9 MB 29.6 MB/s Requirement already satisfied: wrapt~=1.12.1 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-cpu) (1.12.1) Collecting absl-py~=0.10 Downloading absl_py-0.14.0-py3-none-any.whl (131 kB) |████████████████████████████████| 131 kB 34.0 MB/s Collecting keras-preprocessing~=1.1.2 Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB) Collecting opt-einsum~=3.3.0 Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB) Collecting numpy~=1.19.2 Downloading numpy-1.19.5-cp38-cp38-macosx_10_9_x86_64.whl (15.6 MB) |████████████████████████████████| 15.6 MB 28.5 MB/s Collecting tensorboard~=2.6 Using cached tensorboard-2.6.0-py3-none-any.whl (5.6 MB) Collecting grpcio&lt;2.0,&gt;=1.37.0 Downloading grpcio-1.40.0-cp38-cp38-macosx_10_10_x86_64.whl (4.0 MB) |████████████████████████████████| 4.0 MB 20.9 MB/s Collecting tensorflow-estimator~=2.6 Using cached tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB) Collecting protobuf&gt;=3.9.2 Downloading protobuf-3.18.0-cp38-cp38-macosx_10_9_x86_64.whl (1.0 MB) |████████████████████████████████| 1.0 MB 27.9 MB/s Collecting flatbuffers~=1.12.0 Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB) Requirement already satisfied: six~=1.15.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-cpu) (1.15.0) Collecting clang~=5.0 Using cached clang-5.0.tar.gz (30 kB) Requirement already satisfied: requests&lt;3,&gt;=2.21.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6-&gt;tensorflow-cpu) (2.25.1) Requirement already satisfied: werkzeug&gt;=0.11.15 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6-&gt;tensorflow-cpu) (1.0.1) Collecting markdown&gt;=2.6.8 Using cached Markdown-3.3.4-py3-none-any.whl (97 kB) Requirement already satisfied: setuptools&gt;=41.0.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6-&gt;tensorflow-cpu) (52.0.0.post20210125) Collecting tensorboard-plugin-wit&gt;=1.6.0 Using cached tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB) Collecting google-auth-oauthlib&lt;0.5,&gt;=0.4.1 Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB) Collecting tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 Using cached tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB) Collecting google-auth&lt;2,&gt;=1.6.3 Using cached google_auth-1.35.0-py2.py3-none-any.whl (152 kB) Collecting rsa&lt;5,&gt;=3.1.4 Using cached rsa-4.7.2-py3-none-any.whl (34 kB) Collecting cachetools&lt;5.0,&gt;=2.0.0 Using cached cachetools-4.2.2-py3-none-any.whl (11 kB) Collecting pyasn1-modules&gt;=0.2.1 Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB) Collecting requests-oauthlib&gt;=0.7.0 Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB) Collecting pyasn1&lt;0.5.0,&gt;=0.4.6 Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB) Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard~=2.6-&gt;tensorflow-cpu) (4.0.0) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard~=2.6-&gt;tensorflow-cpu) (1.26.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard~=2.6-&gt;tensorflow-cpu) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard~=2.6-&gt;tensorflow-cpu) (2020.12.5) Collecting oauthlib&gt;=3.0.0 Using cached oauthlib-3.1.1-py2.py3-none-any.whl (146 kB) Building wheels for collected packages: clang, termcolor Building wheel for clang (setup.py) ... done Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30702 sha256=da15ad6b0d6d4b2a3771249b17258a29cd98a0ec320998b59d0271947a6dc249 Stored in directory: /Users/adrianmoses/Library/Caches/pip/wheels/f1/60/77/22b9b5887bd47801796a856f47650d9789c74dc3161a26d608 Building wheel for termcolor (setup.py) ... done Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=0e700811fb720cbb45531971531821ac22f2c4752b8e8da9f111d274868e236f Stored in directory: /Users/adrianmoses/Library/Caches/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501 Successfully built clang termcolor Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, clang, astunparse, tensorflow-cpu Attempting uninstall: numpy Found existing installation: numpy 1.20.1 Uninstalling numpy-1.20.1: Successfully uninstalled numpy-1.20.1 Attempting uninstall: h5py Found existing installation: h5py 2.10.0 Uninstalling h5py-2.10.0: Successfully uninstalled h5py-2.10.0 Successfully installed absl-py-0.14.0 astunparse-1.6.3 cachetools-4.2.2 clang-5.0 flatbuffers-1.12 gast-0.4.0 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.40.0 h5py-3.1.0 keras-2.6.0 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.18.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.6.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-cpu-2.6.0 tensorflow-estimator-2.6.0 termcolor-1.1.0 . import tensorflow as tf import numpy as np from tensorflow.keras import Sequential from tensorflow.keras.layers import Dense from tensorflow import keras . data = tf.keras.datasets.fashion_mnist # dataset (training_images, training_labels), (test_images, test_labels) = data.load_data() # load data into train and test sets training_images = training_images / 255.0 # normalize the images so that they&#39;re all between 0 and 1 test_images = test_images / 255.0 model = keras.Sequential([ keras.layers.Flatten(input_shape=(28, 28)), keras.layers.Dense(128, activation=tf.nn.relu), # hidden layer keras.layers.Dense(10, activation=tf.nn.softmax) # output layer ]) model.compile(optimizer=&#39;adam&#39;, # adam is an evolution of sgd to better find that global optimum (uses momentum) loss=&#39;sparse_categorical_crossentropy&#39;, # common loss function for softmax classification metrics=[&#39;accuracy&#39;]) model.fit(training_images, training_labels, epochs=5) . Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz 32768/29515 [=================================] - 0s 0us/step 40960/29515 [=========================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz 26427392/26421880 [==============================] - 1s 0us/step 26435584/26421880 [==============================] - 1s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz 16384/5148 [===============================================================================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz 4423680/4422102 [==============================] - 0s 0us/step 4431872/4422102 [==============================] - 0s 0us/step Epoch 1/5 1875/1875 [==============================] - 2s 732us/step - loss: 0.4964 - accuracy: 0.8251 Epoch 2/5 1875/1875 [==============================] - 1s 745us/step - loss: 0.3719 - accuracy: 0.8655 Epoch 3/5 1875/1875 [==============================] - 1s 740us/step - loss: 0.3349 - accuracy: 0.8771 Epoch 4/5 1875/1875 [==============================] - 1s 743us/step - loss: 0.3114 - accuracy: 0.8846 Epoch 5/5 1875/1875 [==============================] - 1s 747us/step - loss: 0.2940 - accuracy: 0.8915 . &lt;keras.callbacks.History at 0x7ff180afcd00&gt; . model.evaluate(test_images, test_labels) . 313/313 [==============================] - 0s 508us/step - loss: 0.3397 - accuracy: 0.8791 . [0.3397229313850403, 0.8791000247001648] . classifications = model.predict(test_images) print(classifications[0]) # probabilities for each class print(test_labels[0]) # actual correct class . [1.3823378e-05 3.3052868e-07 1.6802342e-05 1.0910228e-06 6.6072988e-07 5.4068407e-03 1.5284693e-05 7.9095788e-02 3.6717476e-05 9.1541272e-01] 9 . model.fit(training_images, training_labels, epochs=50) . Epoch 1/50 1875/1875 [==============================] - 1s 749us/step - loss: 0.2797 - accuracy: 0.8959 Epoch 2/50 1875/1875 [==============================] - 1s 749us/step - loss: 0.2682 - accuracy: 0.9002 Epoch 3/50 1875/1875 [==============================] - 1s 771us/step - loss: 0.2574 - accuracy: 0.9042 Epoch 4/50 1875/1875 [==============================] - 1s 746us/step - loss: 0.2477 - accuracy: 0.9087 Epoch 5/50 1875/1875 [==============================] - 1s 746us/step - loss: 0.2379 - accuracy: 0.9117 Epoch 6/50 1875/1875 [==============================] - 1s 749us/step - loss: 0.2321 - accuracy: 0.9139 Epoch 7/50 1875/1875 [==============================] - 1s 747us/step - loss: 0.2225 - accuracy: 0.9169 Epoch 8/50 1875/1875 [==============================] - 1s 770us/step - loss: 0.2172 - accuracy: 0.9186 Epoch 9/50 1875/1875 [==============================] - 1s 770us/step - loss: 0.2100 - accuracy: 0.9210 Epoch 10/50 1875/1875 [==============================] - 1s 775us/step - loss: 0.2045 - accuracy: 0.9230 Epoch 11/50 1875/1875 [==============================] - 1s 763us/step - loss: 0.1986 - accuracy: 0.9247 Epoch 12/50 1875/1875 [==============================] - 1s 774us/step - loss: 0.1913 - accuracy: 0.9276 Epoch 13/50 1875/1875 [==============================] - 1s 749us/step - loss: 0.1880 - accuracy: 0.9282 Epoch 14/50 1875/1875 [==============================] - 1s 757us/step - loss: 0.1821 - accuracy: 0.9323 Epoch 15/50 1875/1875 [==============================] - 1s 761us/step - loss: 0.1780 - accuracy: 0.9326 Epoch 16/50 1875/1875 [==============================] - 1s 754us/step - loss: 0.1728 - accuracy: 0.9335 Epoch 17/50 1875/1875 [==============================] - 1s 768us/step - loss: 0.1700 - accuracy: 0.9364 Epoch 18/50 1875/1875 [==============================] - 1s 762us/step - loss: 0.1655 - accuracy: 0.9370 Epoch 19/50 1875/1875 [==============================] - 1s 768us/step - loss: 0.1602 - accuracy: 0.9388 Epoch 20/50 1875/1875 [==============================] - 1s 763us/step - loss: 0.1553 - accuracy: 0.9418 Epoch 21/50 1875/1875 [==============================] - 1s 761us/step - loss: 0.1539 - accuracy: 0.9417 Epoch 22/50 1875/1875 [==============================] - 1s 770us/step - loss: 0.1505 - accuracy: 0.9436 Epoch 23/50 1875/1875 [==============================] - 1s 765us/step - loss: 0.1468 - accuracy: 0.9444 Epoch 24/50 1875/1875 [==============================] - 1s 758us/step - loss: 0.1447 - accuracy: 0.9444 Epoch 25/50 1875/1875 [==============================] - 1s 795us/step - loss: 0.1399 - accuracy: 0.9472 Epoch 26/50 1875/1875 [==============================] - 1s 763us/step - loss: 0.1378 - accuracy: 0.9468 Epoch 27/50 1875/1875 [==============================] - 1s 764us/step - loss: 0.1340 - accuracy: 0.9502 Epoch 28/50 1875/1875 [==============================] - 1s 762us/step - loss: 0.1305 - accuracy: 0.9508 Epoch 29/50 1875/1875 [==============================] - 1s 757us/step - loss: 0.1293 - accuracy: 0.9516 Epoch 30/50 1875/1875 [==============================] - 1s 771us/step - loss: 0.1275 - accuracy: 0.9526 Epoch 31/50 1875/1875 [==============================] - 1s 755us/step - loss: 0.1228 - accuracy: 0.9532 Epoch 32/50 1875/1875 [==============================] - 1s 764us/step - loss: 0.1208 - accuracy: 0.9543 Epoch 33/50 1875/1875 [==============================] - 1s 752us/step - loss: 0.1166 - accuracy: 0.9552 Epoch 34/50 1875/1875 [==============================] - 1s 758us/step - loss: 0.1175 - accuracy: 0.9551 Epoch 35/50 1875/1875 [==============================] - 1s 767us/step - loss: 0.1173 - accuracy: 0.9556 Epoch 36/50 1875/1875 [==============================] - 1s 757us/step - loss: 0.1130 - accuracy: 0.9572 Epoch 37/50 1875/1875 [==============================] - 1s 763us/step - loss: 0.1120 - accuracy: 0.9571 Epoch 38/50 1875/1875 [==============================] - 1s 766us/step - loss: 0.1090 - accuracy: 0.9591 Epoch 39/50 1875/1875 [==============================] - 1s 777us/step - loss: 0.1082 - accuracy: 0.9598 Epoch 40/50 1875/1875 [==============================] - 1s 741us/step - loss: 0.1034 - accuracy: 0.9603 Epoch 41/50 1875/1875 [==============================] - 1s 770us/step - loss: 0.1031 - accuracy: 0.9622 Epoch 42/50 1875/1875 [==============================] - 1s 753us/step - loss: 0.1002 - accuracy: 0.9621 Epoch 43/50 1875/1875 [==============================] - 1s 740us/step - loss: 0.1014 - accuracy: 0.9619 Epoch 44/50 1875/1875 [==============================] - 1s 743us/step - loss: 0.0974 - accuracy: 0.9627 Epoch 45/50 1875/1875 [==============================] - 1s 747us/step - loss: 0.0962 - accuracy: 0.9638 Epoch 46/50 1875/1875 [==============================] - 1s 768us/step - loss: 0.0959 - accuracy: 0.9635 Epoch 47/50 1875/1875 [==============================] - 2s 811us/step - loss: 0.0923 - accuracy: 0.9646 Epoch 48/50 1875/1875 [==============================] - 2s 810us/step - loss: 0.0910 - accuracy: 0.9653 Epoch 49/50 1875/1875 [==============================] - 1s 787us/step - loss: 0.0909 - accuracy: 0.9661 Epoch 50/50 1875/1875 [==============================] - 2s 884us/step - loss: 0.0879 - accuracy: 0.9676 . &lt;keras.callbacks.History at 0x7ff1a125fa00&gt; . model.evaluate(test_images, test_labels) . 313/313 [==============================] - 0s 559us/step - loss: 0.5352 - accuracy: 0.8879 . [0.5351569056510925, 0.8878999948501587] . After adding 50 epochs, the training accuracy increased, but the evaluation decreased. This is a sign of overfitting because the model is having a harding time generalizing on data it hasn&#39;t seen . By the way, always recompile the model when retraining. . Let&#39;s use callbacks to train the same model but stopping it when an accuracy has been reached. . class myCallback(keras.callbacks.Callback): def on_epoch_end(self, epoch, logs={}): if logs.get(&#39;accuracy&#39;) &gt; 0.95: print(&quot; nReached 95% accuracy so cancelling training!&quot;) self.model.stop_training = True callbacks = myCallback() model = keras.models.Sequential([ keras.layers.Flatten(), keras.layers.Dense(128, activation=tf.nn.relu), keras.layers.Dense(10, activation=tf.nn.softmax) ]) model.compile(optimizer=&#39;adam&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.fit(training_images, training_labels, epochs=50, callbacks=[callbacks]) . Epoch 1/50 1875/1875 [==============================] - 2s 756us/step - loss: 0.4989 - accuracy: 0.8232 Epoch 2/50 1875/1875 [==============================] - 1s 756us/step - loss: 0.3711 - accuracy: 0.8649 Epoch 3/50 1875/1875 [==============================] - 1s 756us/step - loss: 0.3344 - accuracy: 0.8776 Epoch 4/50 1875/1875 [==============================] - 1s 749us/step - loss: 0.3114 - accuracy: 0.8853 Epoch 5/50 1875/1875 [==============================] - 1s 756us/step - loss: 0.2932 - accuracy: 0.8913 Epoch 6/50 1875/1875 [==============================] - 1s 747us/step - loss: 0.2788 - accuracy: 0.8967 Epoch 7/50 1875/1875 [==============================] - 1s 750us/step - loss: 0.2646 - accuracy: 0.9012 Epoch 8/50 1875/1875 [==============================] - 1s 761us/step - loss: 0.2573 - accuracy: 0.9042 Epoch 9/50 1875/1875 [==============================] - 1s 752us/step - loss: 0.2458 - accuracy: 0.9086 Epoch 10/50 1875/1875 [==============================] - 1s 778us/step - loss: 0.2399 - accuracy: 0.9111 Epoch 11/50 1875/1875 [==============================] - 1s 764us/step - loss: 0.2291 - accuracy: 0.9140 Epoch 12/50 1875/1875 [==============================] - 1s 759us/step - loss: 0.2235 - accuracy: 0.9167 Epoch 13/50 1875/1875 [==============================] - 1s 768us/step - loss: 0.2171 - accuracy: 0.9190 Epoch 14/50 1875/1875 [==============================] - 1s 756us/step - loss: 0.2120 - accuracy: 0.9210 Epoch 15/50 1875/1875 [==============================] - 1s 795us/step - loss: 0.2069 - accuracy: 0.9226 Epoch 16/50 1875/1875 [==============================] - 1s 778us/step - loss: 0.1991 - accuracy: 0.9255 Epoch 17/50 1875/1875 [==============================] - 1s 779us/step - loss: 0.1936 - accuracy: 0.9273 Epoch 18/50 1875/1875 [==============================] - 1s 758us/step - loss: 0.1892 - accuracy: 0.9290 Epoch 19/50 1875/1875 [==============================] - 1s 771us/step - loss: 0.1844 - accuracy: 0.9310 Epoch 20/50 1875/1875 [==============================] - 1s 762us/step - loss: 0.1793 - accuracy: 0.9328 Epoch 21/50 1875/1875 [==============================] - 1s 762us/step - loss: 0.1762 - accuracy: 0.9338 Epoch 22/50 1875/1875 [==============================] - 1s 758us/step - loss: 0.1702 - accuracy: 0.9360 Epoch 23/50 1875/1875 [==============================] - 1s 771us/step - loss: 0.1670 - accuracy: 0.9373 Epoch 24/50 1875/1875 [==============================] - 2s 800us/step - loss: 0.1628 - accuracy: 0.9391 Epoch 25/50 1875/1875 [==============================] - 1s 761us/step - loss: 0.1600 - accuracy: 0.9400 Epoch 26/50 1875/1875 [==============================] - 1s 762us/step - loss: 0.1559 - accuracy: 0.9410 Epoch 27/50 1875/1875 [==============================] - 1s 757us/step - loss: 0.1526 - accuracy: 0.9418 Epoch 28/50 1875/1875 [==============================] - 1s 768us/step - loss: 0.1479 - accuracy: 0.9440 Epoch 29/50 1875/1875 [==============================] - 1s 760us/step - loss: 0.1463 - accuracy: 0.9447 Epoch 30/50 1875/1875 [==============================] - 1s 785us/step - loss: 0.1409 - accuracy: 0.9468 Epoch 31/50 1875/1875 [==============================] - 1s 768us/step - loss: 0.1395 - accuracy: 0.9477 Epoch 32/50 1875/1875 [==============================] - 1s 798us/step - loss: 0.1371 - accuracy: 0.9486 Epoch 33/50 1875/1875 [==============================] - 1s 754us/step - loss: 0.1353 - accuracy: 0.9484 Epoch 34/50 1875/1875 [==============================] - 1s 760us/step - loss: 0.1303 - accuracy: 0.9503 Reached 95% accuracy so cancelling training! . &lt;keras.callbacks.History at 0x7ff1a1261730&gt; . Callbacks are cool! . Next chapter will use the more efficient approach of convolutions .",
            "url": "https://adrianmoses.github.io/ml-journal/2021/09/24/ai-and-ml-for-coders-ch2.html",
            "relUrl": "/2021/09/24/ai-and-ml-for-coders-ch2.html",
            "date": " • Sep 24, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "AI and ML for Coders",
            "content": "import tensorflow as tf import numpy as np from tensorflow.keras import Sequential from tensorflow.keras.layers import Dense l0 = Dense(units=1, input_shape=[1]) model = Sequential([l0]) model.compile(optimizer=&#39;sgd&#39;, loss=&#39;mean_squared_error&#39;) xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float) ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float) model.fit(xs, ys, epochs=500) print(model.predict([10.0])) print(&quot;Here&#39;s what I learned: {}&quot;.format(l0.get_weights())) .",
            "url": "https://adrianmoses.github.io/ml-journal/2021/09/24/ai-and-ml-for-coders-ch1.html",
            "relUrl": "/2021/09/24/ai-and-ml-for-coders-ch1.html",
            "date": " • Sep 24, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Mastering Spacy Ch. 1",
            "content": "import spacy . !python -m spacy download en . ⚠ As of spaCy v3.0, shortcuts like &#39;en&#39; are deprecated. Please use the full pipeline package name &#39;en_core_web_sm&#39; instead. Collecting en-core-web-sm==3.1.0 Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl (13.6 MB) |████████████████████████████████| 13.6 MB 7.4 MB/s Requirement already satisfied: spacy&lt;3.2.0,&gt;=3.1.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from en-core-web-sm==3.1.0) (3.1.2) Requirement already satisfied: typer&lt;0.4.0,&gt;=0.3.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (0.3.2) Requirement already satisfied: packaging&gt;=20.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (20.9) Requirement already satisfied: numpy&gt;=1.15.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (1.20.1) Requirement already satisfied: setuptools in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (52.0.0.post20210125) Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (3.0.5) Requirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.1 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (2.4.1) Requirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.4 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (2.0.6) Requirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.7 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (3.0.8) Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (1.0.5) Requirement already satisfied: thinc&lt;8.1.0,&gt;=8.0.8 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (8.0.10) Requirement already satisfied: pydantic!=1.8,!=1.8.1,&lt;1.9.0,&gt;=1.7.4 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (1.8.2) Requirement already satisfied: blis&lt;0.8.0,&gt;=0.4.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (0.7.4) Requirement already satisfied: pathy&gt;=0.3.5 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (0.6.0) Requirement already satisfied: jinja2 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (2.11.3) Requirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (2.25.1) Requirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (4.59.0) Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (2.0.5) Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.8.1 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (0.8.2) Requirement already satisfied: pyparsing&gt;=2.0.2 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from packaging&gt;=20.0-&gt;spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (2.4.7) Requirement already satisfied: smart-open&lt;6.0.0,&gt;=5.0.0 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from pathy&gt;=0.3.5-&gt;spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (5.2.1) Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,&lt;1.9.0,&gt;=1.7.4-&gt;spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (3.7.4.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (2020.12.5) Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (4.0.0) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (1.26.4) Requirement already satisfied: click&lt;7.2.0,&gt;=7.1.1 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from typer&lt;0.4.0,&gt;=0.3.0-&gt;spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (7.1.2) Requirement already satisfied: MarkupSafe&gt;=0.23 in /Users/adrianmoses/opt/anaconda3/lib/python3.8/site-packages (from jinja2-&gt;spacy&lt;3.2.0,&gt;=3.1.0-&gt;en-core-web-sm==3.1.0) (1.1.1) ✔ Download and installation successful You can now load the package via spacy.load(&#39;en_core_web_sm&#39;) . nlp = spacy.load(&#39;en_core_web_sm&#39;) doc = nlp(&#39;I have a ginger cat.&#39;) . doc . I have a ginger cat. . DisplaCy . Let&#39;s take a look at the POS tagging and tokens with the Dispacy graphics . from spacy import displacy doc = nlp(&#39;Bill Gates is the CEO of Microsoft&#39;) displacy.render(doc, style=&#39;dep&#39;) . Bill PROPN Gates PROPN is AUX the DET CEO NOUN of ADP Microsoft PROPN compound nsubj det attr prep pobj",
            "url": "https://adrianmoses.github.io/ml-journal/2021/09/23/mastering-spacy-chapter-1.html",
            "relUrl": "/2021/09/23/mastering-spacy-chapter-1.html",
            "date": " • Sep 23, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Intro to NLP",
            "content": "This course appears a bit more hands-on as it starts off with introducing spacy. spacy.load code examples are added . !pip install spacy !python -m spacy download en . import spacy nlp = spacy.load(&#39;en_core_web_sm&#39;) . doc = nlp(&quot;Tea is healthy and calming, don&#39;t you think?&quot;) . for token in doc: print(token) . Tea is healthy and calming , do n&#39;t you think ? . These are token objects. A token object has the lemmatization with token.lemma_ and if it&#39;s a stop word token.is_stop . print(f&quot;Token t tLemma t tStopword&quot;.format(&#39;Token&#39;, &#39;Lemma&#39;, &#39;Stopword&#39;)) print(&quot;-&quot;*40) for token in doc: print(f&quot;{str(token)} t t{token.lemma_} t t{token.is_stop}&quot;) . Token Lemma Stopword - Tea tea False is be True healthy healthy False and and True calming calm False , , False do do True n&#39;t n&#39;t True you you True think think False ? ? False . Lemmatization and Stopwords can be helpful, but also detrimental to a model&#39;s performance. Consider it as hyperparameters for tweaking the performance of a model. . Pattern Matching . Spacy has pattern matching capabilities that are easier to use then Regex . from spacy.matcher import PhraseMatcher matcher = PhraseMatcher(nlp.vocab, attr=&#39;LOWER&#39;) . Matchers depend on a vocabulary model, so the english model above was used. attr=&#39;LOWER&#39; lowers all text ensuring case insentivity . Convert the terms we need to match to documents and add to the matcher . terms = [&#39;Galaxy Note&#39;, &#39;iPhone 11&#39;, &#39;iPhone XS&#39;, &#39;Google Pixel&#39;] patterns = [nlp(text) for text in terms] matcher.add(&quot;TerminologyList&quot;, patterns) . text_doc = nlp(&quot;Glowing review overall, and some really interesting side-by-side &quot; &quot;photography tests pitting the iPhone 11 Pro against the &quot; &quot;Galaxy Note 10 Plus and last year&#39;s iPhone XS and Google Pixel 3.&quot;) matches = matcher(text_doc) print(matches) . [(3766102292120407359, 17, 19), (3766102292120407359, 22, 24), (3766102292120407359, 30, 32), (3766102292120407359, 33, 35)] . A match is a tuple of (match_id, start_pos, end_pos) . match_id, start, end = matches[0] print(nlp.vocab.strings[match_id], text_doc[start:end]) . TerminologyList iPhone 11 . Text Classification . Machines need numeric representations of text. . One way to convert a sentence or phrase to a numeric represneataiotion is to count the occurances of a word in a document Then the vector is the length of every word in the entire corpus. A variation of one-hot encoding. . This is called bag of words . Another approach is by scaling the term count by the overall term&#39;s frequency in the corpus. The name of that representation is called TF-IDF or Term Frequency - Inverse Document Frequency . spacy can support bag of words with the TextCategorizer . nlp = spacy.blank(&quot;en&quot;) textcat = nlp.create_pipe(&quot;textcat&quot;, config={ &quot;exclusive_classes&quot;: True, &quot;architecture&quot;: &quot;bow&quot; }) nlp.add_pipe(textcat) . ConfigValidationError Traceback (most recent call last) &lt;ipython-input-14-b892f9c822da&gt; in &lt;module&gt; 1 nlp = spacy.blank(&#34;en&#34;) 2 -&gt; 3 textcat = nlp.create_pipe(&#34;textcat&#34;, config={ 4 &#34;exclusive_classes&#34;: True, 5 &#34;architecture&#34;: &#34;bow&#34; ~/opt/anaconda3/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate) 659 # We&#39;re calling the internal _fill here to avoid constructing the 660 # registered functions twice --&gt; 661 resolved = registry.resolve(cfg, validate=validate) 662 filled = registry.fill({&#34;cfg&#34;: cfg[factory_name]}, validate=validate)[&#34;cfg&#34;] 663 filled = Config(filled) ~/opt/anaconda3/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate) 727 validate: bool = True, 728 ) -&gt; Dict[str, Any]: --&gt; 729 resolved, _ = cls._make( 730 config, schema=schema, overrides=overrides, validate=validate, resolve=True 731 ) ~/opt/anaconda3/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate) 776 if not is_interpolated: 777 config = Config(orig_config).interpolate() --&gt; 778 filled, _, resolved = cls._fill( 779 config, schema, validate=validate, overrides=overrides, resolve=resolve 780 ) ~/opt/anaconda3/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides) 831 schema.__fields__[key] = copy_model_field(field, Any) 832 promise_schema = cls.make_promise_schema(value, resolve=resolve) --&gt; 833 filled[key], validation[v_key], final[key] = cls._fill( 834 value, 835 promise_schema, ~/opt/anaconda3/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides) 897 result = schema.parse_obj(validation) 898 except ValidationError as e: --&gt; 899 raise ConfigValidationError( 900 config=config, errors=e.errors(), parent=parent 901 ) from None ConfigValidationError: Config validation error textcat -&gt; architecture extra fields not permitted textcat -&gt; exclusive_classes extra fields not permitted {&#39;nlp&#39;: &lt;spacy.lang.en.English object at 0x7fdf109c6fa0&gt;, &#39;name&#39;: &#39;textcat&#39;, &#39;architecture&#39;: &#39;bow&#39;, &#39;exclusive_classes&#39;: True, &#39;model&#39;: {&#39;@architectures&#39;: &#39;spacy.TextCatEnsemble.v2&#39;, &#39;linear_model&#39;: {&#39;@architectures&#39;: &#39;spacy.TextCatBOW.v2&#39;, &#39;exclusive_classes&#39;: True, &#39;ngram_size&#39;: 1, &#39;no_output_layer&#39;: False}, &#39;tok2vec&#39;: {&#39;@architectures&#39;: &#39;spacy.Tok2Vec.v2&#39;, &#39;embed&#39;: {&#39;@architectures&#39;: &#39;spacy.MultiHashEmbed.v2&#39;, &#39;width&#39;: 64, &#39;rows&#39;: [2000, 2000, 1000, 1000, 1000, 1000], &#39;attrs&#39;: [&#39;ORTH&#39;, &#39;LOWER&#39;, &#39;PREFIX&#39;, &#39;SUFFIX&#39;, &#39;SHAPE&#39;, &#39;ID&#39;], &#39;include_static_vectors&#39;: False}, &#39;encode&#39;: {&#39;@architectures&#39;: &#39;spacy.MaxoutWindowEncoder.v2&#39;, &#39;width&#39;: 64, &#39;window_size&#39;: 1, &#39;maxout_pieces&#39;: 3, &#39;depth&#39;: 2}}}, &#39;threshold&#39;: 0.5, &#39;@factories&#39;: &#39;textcat&#39;} .",
            "url": "https://adrianmoses.github.io/ml-journal/jupyter/2021/09/18/intro-to-nlp.html",
            "relUrl": "/jupyter/2021/09/18/intro-to-nlp.html",
            "date": " • Sep 18, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://adrianmoses.github.io/ml-journal/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://adrianmoses.github.io/ml-journal/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}